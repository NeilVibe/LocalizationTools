# Testing Framework - LocalizationTools

**Comprehensive, organized testing for ALL components**

---

## ğŸ¯ Testing Philosophy

**We test EVERYTHING:**
- âœ… Every function, every feature
- âœ… Backend code (server, API, database)
- âœ… Frontend code (client, UI, tools)
- âœ… Network communication (client-server)
- âœ… File operations (upload, download, processing)
- âœ… Logging and statistics
- âœ… Error handling and edge cases
- âœ… Performance and robustness

**No bloat allowed. CLEAN, organized tests.**

---

## ğŸ“ Test Structure (CLEAN Organization)

```
tests/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ conftest.py                  # Shared fixtures and config
â”‚
â”œâ”€â”€ unit/                        # Unit tests (fast, isolated)
â”‚   â”œâ”€â”€ client/
â”‚   â”‚   â”œâ”€â”€ test_utils_logger.py
â”‚   â”‚   â”œâ”€â”€ test_utils_progress.py
â”‚   â”‚   â”œâ”€â”€ test_utils_file_handler.py
â”‚   â”‚   â”œâ”€â”€ test_xls_transfer_core.py
â”‚   â”‚   â”œâ”€â”€ test_xls_transfer_embeddings.py
â”‚   â”‚   â”œâ”€â”€ test_xls_transfer_translation.py
â”‚   â”‚   â””â”€â”€ test_xls_transfer_excel_utils.py
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ test_api_logs.py
â”‚       â”œâ”€â”€ test_api_auth.py
â”‚       â”œâ”€â”€ test_api_stats.py
â”‚       â”œâ”€â”€ test_database_models.py
â”‚       â””â”€â”€ test_database_crud.py
â”‚
â”œâ”€â”€ integration/                 # Integration tests (multiple components)
â”‚   â”œâ”€â”€ client/
â”‚   â”‚   â”œâ”€â”€ test_xls_transfer_workflow.py
â”‚   â”‚   â”œâ”€â”€ test_logging_integration.py
â”‚   â”‚   â””â”€â”€ test_ui_integration.py
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ test_api_endpoints.py
â”‚       â”œâ”€â”€ test_database_operations.py
â”‚       â””â”€â”€ test_stats_aggregation.py
â”‚
â”œâ”€â”€ e2e/                         # End-to-end tests (full workflows)
â”‚   â”œâ”€â”€ client/
â”‚   â”‚   â””â”€â”€ test_full_tool_workflow.py
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ test_full_server_workflow.py
â”‚
â”œâ”€â”€ fixtures/                    # Test data and fixtures
â”‚   â”œâ”€â”€ sample_excel_files/
â”‚   â”œâ”€â”€ sample_models/
â”‚   â””â”€â”€ sample_configs/
â”‚
â””â”€â”€ helpers/                     # Test utilities
    â”œâ”€â”€ mock_server.py
    â”œâ”€â”€ test_data_generators.py
    â””â”€â”€ assertions.py
```

---

## ğŸƒ Running Tests

### **Run All Tests**
```bash
pytest
```

### **Run by Type**
```bash
# Unit tests only (fast)
pytest -m unit

# Integration tests
pytest -m integration

# End-to-end tests
pytest -m e2e

# Client tests only
pytest -m client

# Server tests only
pytest -m server

# Database tests
pytest -m database

# Network tests
pytest -m network
```

### **Run by Component**
```bash
# All client tests
pytest tests/unit/client/

# All server tests
pytest tests/unit/server/

# Specific module
pytest tests/unit/client/test_utils_logger.py

# Specific test
pytest tests/unit/client/test_utils_logger.py::test_log_operation
```

### **Run with Coverage**
```bash
# Generate coverage report
pytest --cov=client --cov=server --cov-report=html

# View coverage report
open htmlcov/index.html  # or xdg-open on Linux
```

### **Run in Watch Mode** (for development)
```bash
# Install pytest-watch
pip install pytest-watch

# Auto-run tests on file changes
ptw
```

---

## âœ… Test Requirements

### **Every Test Must Have:**
1. **Clear docstring** - What it tests and why
2. **Descriptive name** - `test_function_does_what_when_condition`
3. **Arrange-Act-Assert** pattern
4. **Proper markers** - @pytest.mark.unit, etc.
5. **Clean fixtures** - Use conftest.py
6. **No hardcoded paths** - Use fixtures and Path()
7. **Cleanup** - Remove temp files after test

### **Example of CLEAN Test:**
```python
import pytest
from pathlib import Path

@pytest.mark.unit
@pytest.mark.client
def test_file_handler_validates_existing_file(tmp_path):
    """
    Test that file_handler correctly validates an existing file.

    Given: A temporary file exists
    When: validate_file_exists() is called
    Then: Returns (True, "") indicating success
    """
    # Arrange
    test_file = tmp_path / "test.xlsx"
    test_file.write_text("test content")

    # Act
    from server.utils.client.file_handler import validate_file_exists
    is_valid, error = validate_file_exists(str(test_file), ['.xlsx'])

    # Assert
    assert is_valid is True
    assert error == ""
```

---

## ğŸ§ª Test Coverage Goals

**Minimum Coverage**: 80% (enforced by pytest.ini)
**Target Coverage**: 90%+

### **Coverage by Component:**
- **Utilities**: 95%+ (critical reusable code)
- **Tools (XLSTransfer, etc.)**: 90%+
- **Server API**: 90%+
- **Database**: 90%+
- **UI**: 70%+ (harder to test, but still important)

---

## ğŸ” What We Test

### **1. Unit Tests** (Fast, Isolated)

**Client Utils:**
- âœ… Logger sends correct data format
- âœ… Logger queues when server unavailable
- âœ… Progress tracker updates correctly
- âœ… File handler validates files properly
- âœ… File handler creates safe filenames
- âœ… File handler manages temp files

**XLSTransfer:**
- âœ… Text cleaning removes unwanted characters
- âœ… Column conversion works correctly
- âœ… Code pattern detection finds all codes
- âœ… Embedding generation produces correct shapes
- âœ… FAISS index creation works
- âœ… Translation matching returns correct results
- âœ… Excel reading handles all formats

**Server:**
- âœ… API endpoints validate input
- âœ… Database models have correct fields
- âœ… CRUD operations work correctly
- âœ… Authentication validates credentials
- âœ… Stats aggregation calculates correctly

### **2. Integration Tests** (Multiple Components)

**Client:**
- âœ… Logger + Progress work together
- âœ… File upload â†’ Processing â†’ Download workflow
- âœ… XLSTransfer full dictionary creation
- âœ… XLSTransfer full translation workflow
- âœ… UI updates when processing completes

**Server:**
- âœ… API â†’ Database â†’ Response flow
- âœ… Log entry â†’ Stats aggregation â†’ Dashboard
- âœ… Authentication â†’ Session â†’ Logging
- âœ… Multiple concurrent users

### **3. End-to-End Tests** (Full Workflows)

**Complete User Flows:**
- âœ… User logs in â†’ Uses tool â†’ Sees stats
- âœ… Upload file â†’ Process â†’ Download result
- âœ… Create dictionary â†’ Load â†’ Translate
- âœ… Admin views dashboard â†’ Exports report

**Network Tests:**
- âœ… Client connects to server
- âœ… Logs sent successfully
- âœ… Server returns correct responses
- âœ… Handles network errors gracefully

**Database Tests:**
- âœ… Tables created correctly
- âœ… Data persists across connections
- âœ… Queries return correct results
- âœ… Aggregation views work

---

## ğŸ› ï¸ Test Utilities

### **Fixtures** (in conftest.py)

```python
@pytest.fixture
def sample_excel_file(tmp_path):
    """Create a sample Excel file for testing."""
    # Implementation

@pytest.fixture
def mock_server():
    """Mock server for client tests."""
    # Implementation

@pytest.fixture
def test_database():
    """Temporary test database."""
    # Implementation
```

### **Mock Objects**

```python
# Mock Korean BERT model (fast for testing)
@pytest.fixture
def mock_model():
    class MockModel:
        def encode(self, texts):
            return [[0.1] * 384 for _ in texts]
    return MockModel()
```

### **Test Data Generators**

```python
# Generate realistic test data
def generate_excel_with_translations(rows=100):
    # Creates Excel with KR and translation columns
    pass
```

---

## ğŸ“Š Test Reporting

### **After Running Tests:**

**Terminal Output:**
```
tests/unit/client/test_utils_logger.py::test_log_operation PASSED     [ 10%]
tests/unit/client/test_utils_progress.py::test_update PASSED           [ 20%]
...
==================== 50 passed in 2.34s ====================

Coverage: 87%
Missing lines: client/utils/logger.py (45-47, 102)
```

**HTML Coverage Report:**
- Open `htmlcov/index.html` in browser
- See line-by-line coverage
- Identify untested code

### **CI/CD Integration** (Future)

```yaml
# .github/workflows/test.yml
- name: Run tests
  run: pytest

- name: Upload coverage
  run: codecov
```

---

## ğŸ§¹ Keeping Tests CLEAN

**DO:**
- âœ… Use descriptive test names
- âœ… One assertion per test (when possible)
- âœ… Use fixtures for common setup
- âœ… Clean up temp files
- âœ… Mark tests appropriately
- âœ… Document complex tests
- âœ… Keep tests fast (use mocks)

**DON'T:**
- âŒ Hardcode file paths
- âŒ Rely on external services (mock them)
- âŒ Leave temp files behind
- âŒ Write slow tests without @pytest.mark.slow
- âŒ Test implementation details (test behavior)
- âŒ Duplicate test code (use fixtures)
- âŒ Commit failing tests

---

## ğŸ”„ Test-Driven Development (TDD)

**For New Features:**

1. **Write test first** (it will fail)
2. **Write minimal code** to pass
3. **Refactor** while keeping tests green
4. **Repeat**

**Example:**
```python
# 1. Write test (fails)
def test_new_feature():
    result = new_feature(input)
    assert result == expected

# 2. Implement feature (test passes)
def new_feature(input):
    return expected

# 3. Refactor (tests still pass)
def new_feature(input):
    # Cleaner implementation
    return expected
```

---

## ğŸ“ˆ Test Metrics

**We Track:**
- âœ… Test count (goal: 200+ tests)
- âœ… Coverage percentage (goal: 90%+)
- âœ… Test execution time (goal: <30s for unit tests)
- âœ… Flaky tests (goal: 0)
- âœ… Test maintenance (tests updated with code)

---

## ğŸš€ Continuous Testing

**During Development:**
```bash
# Terminal 1: Watch mode
ptw

# Terminal 2: Code changes
# Tests auto-run on save
```

**Before Commit:**
```bash
# Run all tests
pytest

# Check coverage
pytest --cov

# Fix any failures
```

**In CI/CD:**
- All tests run automatically
- Coverage report generated
- Failures block merge

---

## ğŸ“ Test Documentation

**Each test file has:**
- Module docstring explaining what's tested
- Individual test docstrings
- Comments for complex assertions
- Examples of expected behavior

---

**COMPREHENSIVE TESTING = ROBUST CODE** âœ…

**No bloat. CLEAN, organized, thorough.**
