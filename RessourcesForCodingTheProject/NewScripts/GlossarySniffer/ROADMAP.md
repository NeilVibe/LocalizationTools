# GlossarySniffer Roadmap

**Created**: 2025-11-24
**Status**: In Development
**Reference Script**: QuickSearch0818.py (Aho-Corasick glossary extraction)

---

## ğŸ¯ Purpose

**GlossarySniffer** extracts glossary terms from XML files and searches for them in Excel text lines, outputting which glossary terms appear in each line.

### Use Case
- Analyze translation lines to identify glossary terms automatically
- Build glossary from XML `StrOrigin` attributes with smart filtering
- Fast multi-pattern matching using Aho-Corasick algorithm
- Output Excel with original lines + glossary terms found

---

## ğŸ“‹ Input/Output

### Input 1: XML Glossary Source
**Format**: XML file with `StrOrigin` (Korean) and `Str` (English) attributes
```xml
<Texts>
  <Text>
    <LocStr StrOrigin="í´ë¦¬í”„" Str="Kliff"/>
    <LocStr StrOrigin="ì¹¼íŒŒë°" Str="Calphade"/>
    <LocStr StrOrigin="ì—˜ë ˆë…¸ì–´ ê³µì‘" Str="Duke Elenor"/>
    <LocStr StrOrigin="ê³ ê³ êµ¬êµ¬ ë•…" Str="Lands of Gogogugu"/>
  </Text>
</Texts>
```

### Input 2: Lines to Analyze
**Format**: Excel file (`.xlsx`) with Korean lines to check
```
| Line (Korean)                                |
|----------------------------------------------|
| í´ë¦¬í”„ê°€ ì¹¼íŒŒë°ì— ê°€ì„œ ì¹œêµ¬ë“¤ê³¼ ì´ì•¼ê¸°í–ˆë‹¤        |
| ë‚˜ëŠ” ì—˜ë ˆë…¸ì–´ ê³µì‘ì´ê³ , ê³ ê³ êµ¬êµ¬ ë•…ì„ ë‹¤ìŠ¤ë¦°ë‹¤   |
```

### Output: Analysis Result
**Format**: Excel file with original lines + glossary terms found + mapped translations
```
| Original Line (Korean)                                     | Glossary Terms Found (StrOrigin=Korean) | Mapped Translations (Str=English) |
|------------------------------------------------------------|----------------------------------------|-----------------------------------|
| í´ë¦¬í”„ê°€ ì¹¼íŒŒë°ì— ê°€ì„œ ì¹œêµ¬ë“¤ê³¼ ì´ì•¼ê¸°í–ˆë‹¤                      | í´ë¦¬í”„, ì¹¼íŒŒë°                          | Kliff, Calphade                   |
| ë‚˜ëŠ” ì—˜ë ˆë…¸ì–´ ê³µì‘ì´ê³ , ê³ ê³ êµ¬êµ¬ ë•…ì„ ë‹¤ìŠ¤ë¦°ë‹¤                 | ì—˜ë ˆë…¸ì–´ ê³µì‘, ê³ ê³ êµ¬êµ¬ ë•…                | Duke Elenor, Lands of Gogogugu    |
```

**NEW ENHANCEMENT (2025-11-24)**:
- **Column 3 added**: Maps each StrOrigin match to its corresponding Str value from XML
- **Example**: If "í´ë¦¬í”„" (Korean) is found, show both "í´ë¦¬í”„" (StrOrigin) and "Kliff" (Str=English)
- **Use case**: See Korean glossary term AND its English translation side-by-side
- **Direction**: Korean (StrOrigin) â†’ English (Str)

---

## ğŸ”§ Glossary Building Logic - HARDCODED RULES

### âš™ï¸ All Rules Hardcoded (No User Input Needed!)

**Based on QuickSearch0818.py glossary_filter() lines 2113-2149**

### 5 HARDCODED FILTERING RULES:

**Rule 1: Length Threshold**
```python
DEFAULT_LENGTH_THRESHOLD = 15  # Korean names/terms are usually short
```
- âœ… KEEP: Terms < 15 characters
- âŒ SKIP: Anything >= 15 characters (too long, likely a phrase/sentence)

**Rule 2: Minimum Occurrence**
```python
MIN_OCCURRENCE = 2  # Must appear at least 2 times
```
- âœ… KEEP: Terms that appear 2+ times in XML
- âŒ SKIP: One-off terms (likely typos or descriptions, not real glossary)

**Rule 3: No Punctuation Endings**
```python
FILTER_SENTENCES = True
Pattern: r'[.?!]\s*$'  # Ends with .?!
```
- âœ… KEEP: "Kliff", "Duke Elenor"
- âŒ SKIP: "Welcome.", "How are you?", "Hello!"

**Rule 4: No Punctuation Inside**
```python
FILTER_PUNCTUATION = True
Checks: string.punctuation (except spaces/hyphens) + special ellipsis â€¦
```
- âœ… KEEP: "Duke Elenor" (space ok), "Black-Desert" (hyphen ok)
- âŒ SKIP: "Hello, world", "Waitâ€”what", "Well..."

**Rule 5: Non-Empty Only**
- âœ… KEEP: Any non-empty string
- âŒ SKIP: Empty strings, None values

### Two-Pass Filtering (Like QuickSearch0818)
```python
# Pass 1: Basic filtering + count occurrences
for term in all_terms:
    if passes_basic_filters(term):
        count_map[term] += 1

# Pass 2: Apply min_occurrence filter
glossary = [term for term, count in count_map.items() if count >= MIN_OCCURRENCE]

# Sort by frequency (most common first)
glossary.sort(key=count, reverse=True)
```

### Example Filtering
```
INPUT: 1000 StrOrigin entries from XML

âœ… KEEP (111 terms after filtering):
- "í´ë¦¬í”„" (4 chars, appears 5 times)
- "ì¹¼íŒŒë°" (4 chars, appears 3 times)
- "ì—˜ë ˆë…¸ì–´ ê³µì‘" (8 chars, appears 2 times)
- "ê³ ê³ êµ¬êµ¬ ë•…" (7 chars, appears 2 times)
- "ê²€ì€ì‚¬ë§‰" (4 chars, appears 10 times)

âŒ SKIP:
- "í´ë¦¬í”„ê°€ ë„ì‹œì— ê°”ë‹¤." (ends with ., sentence)
- "ì•ˆë…•í•˜ì„¸ìš”, ì—¬í–‰ìë‹˜!" (contains comma and !, punctuation)
- "ì´ê²ƒì€ ë§¤ìš° ê¸´ ì„¤ëª…ì…ë‹ˆë‹¤" (16 chars, too long)
- "ì„ì‹œí•­ëª©" (1 occurrence only, not recurring glossary)
- "..." (only punctuation)
```

### Word Boundary Matching
```python
WORD_BOUNDARIES = True  # Only match complete words
```
- âœ… MATCH: "Duke" in "The Duke arrived"
- âŒ SKIP: "Duke" inside "Archduke" (not at word boundary)

---

## âš¡ Search Algorithm: Aho-Corasick + Word Boundaries

### Why Aho-Corasick?
- **Fast multi-pattern matching**: Search for ALL glossary terms in single pass
- **Efficient**: O(n + m + z) where n=text length, m=total pattern length, z=matches
- **Perfect for glossary**: Can find overlapping patterns
- **Proven in QuickSearch0818**: Already successfully used in production

### Implementation Steps (Enhanced)
1. **Build Automaton**: Add all glossary terms to Aho-Corasick automaton
2. **Make Automaton**: Finalize state machine for fast searching
3. **Scan Each Line**: Use automaton to find all glossary term candidates
4. **Word Boundary Check**: Validate each match is at word boundaries (not inside other words)
5. **Aggregate Results**: Collect all valid unique terms found per line

### Word Boundary Logic (NEW!)
```python
# For each Aho-Corasick match, check:
start_pos = match_end - len(term) + 1

# Character before match (if exists)
if start_pos > 0:
    char_before = line[start_pos - 1]
    if char_before.isalnum():
        skip  # Inside a word!

# Character after match (if exists)
if match_end + 1 < len(line):
    char_after = line[match_end + 1]
    if char_after.isalnum():
        skip  # Inside a word!
```

### Example Search with Word Boundaries
```python
# Glossary terms: ["í´ë¦¬í”„", "ê³µì‘", "ì—˜ë ˆë…¸ì–´ ê³µì‘"]
# Line: "í´ë¦¬í”„ê°€ ì—˜ë ˆë…¸ì–´ ê³µì‘ì„ ë§Œë‚¬ë‹¤"

# Aho-Corasick finds: ["í´ë¦¬í”„", "ê³µì‘", "ì—˜ë ˆë…¸ì–´ ê³µì‘"]
# Word boundary check:
#   - "í´ë¦¬í”„" âœ… (ê°€ is not alnum in this context)
#   - "ê³µì‘" inside "ì—˜ë ˆë…¸ì–´ ê³µì‘" âŒ (prefer longest)
#   - "ì—˜ë ˆë…¸ì–´ ê³µì‘" âœ… (ì„ is not alnum)

# Final output: "í´ë¦¬í”„, ì—˜ë ˆë…¸ì–´ ê³µì‘"
```

### Korean Language Considerations
- Korean doesn't have case sensitivity (no uppercase/lowercase)
- Word boundaries work with particles (ê°€, ì„, ëŠ”, etc.)
- Multi-word Korean terms work fine ("ì—˜ë ˆë…¸ì–´ ê³µì‘")

---

## ğŸ”„ Development Plan

### Phase 1: XML Glossary Extraction âœ… (Target: Complete)
- [ ] Parse XML files using `lxml` or `xml.etree.ElementTree`
- [ ] Extract all `StrOrigin` attributes
- [ ] Apply length threshold filter (default: 30 chars)
- [ ] Remove entries with punctuation
- [ ] Remove full sentences
- [ ] Deduplicate terms
- [ ] Save glossary to internal list

**Reference**: QuickSearch0818.py lines 2113-2149 (glossary_filter function)

### Phase 2: Excel Input Handling âœ… (Target: Complete)
- [ ] Read Excel file with `openpyxl`
- [ ] Extract lines from first column (or user-specified column)
- [ ] Store lines in list for processing

**Reference**: Standard openpyxl patterns from XLSTransfer, QuickSearch

### Phase 3: Aho-Corasick Search Implementation âœ… (Target: Complete)
- [ ] Import `ahocorasick` library
- [ ] Build automaton with all glossary terms
- [ ] For each Excel line:
  - [ ] Run Aho-Corasick search
  - [ ] Collect all matched glossary terms
  - [ ] Handle overlapping matches (e.g., "Duke" vs "Duke Elenor")
- [ ] Prefer longest matches for overlapping terms

**Reference**: QuickSearch0818.py lines 2211-2259 (Aho-Corasick implementation)

### Phase 4: Excel Output Generation âœ… (Target: Complete)
- [ ] Create new Excel workbook
- [ ] Column 1: Original lines
- [ ] Column 2: Glossary terms found (comma-separated)
- [ ] Format: Clean, readable
- [ ] Save to output file

**Reference**: openpyxl workbook creation patterns

### Phase 5: GUI & File Pickers âœ… (Target: Complete)
- [ ] Tkinter file picker for XML glossary source
- [ ] Tkinter file picker for Excel input lines
- [ ] Tkinter save dialog for Excel output
- [ ] Progress messages
- [ ] Error handling

**Reference**: Standard Tkinter patterns from all NewScripts

### Phase 6: Advanced Features (Optional)
- [ ] Configurable length threshold (GUI input)
- [ ] Option to include/exclude punctuation
- [ ] Case-sensitive vs case-insensitive matching
- [ ] Multiple XML source files
- [ ] Column selection for Excel input
- [ ] Statistics (total lines, terms found, coverage %)

---

## ğŸ“¦ Dependencies

```bash
pip install openpyxl lxml pyahocorasick
```

### Libraries Used
- **openpyxl**: Excel file handling (read/write .xlsx)
- **lxml**: Fast XML parsing (preferred) or xml.etree.ElementTree (fallback)
- **pyahocorasick**: Aho-Corasick algorithm for fast multi-pattern matching
- **tkinter**: GUI file pickers (built-in Python)
- **re**: Regular expressions for text cleaning/filtering
- **string**: Punctuation detection for filtering

### Why These Libraries?
- **Aho-Corasick**: 10-100x faster than regex for multi-pattern matching
- **lxml**: Faster than ElementTree for large XML files
- **openpyxl**: Standard Excel library, reliable for .xlsx
- All proven in QuickSearch0818 production use

---

## ğŸ¨ Script Structure

### File Name
`glossary_sniffer_1124.py`

### Main Functions
```python
def extract_glossary_from_xml(xml_path, length_threshold=30):
    """
    Extract glossary terms from XML StrOrigin attributes
    Returns: list of glossary terms
    """

def filter_glossary_terms(terms, length_threshold):
    """
    Filter out non-glossary entries (punctuation, long phrases, etc.)
    Returns: filtered list of terms
    """

def build_ahocorasick_automaton(glossary_terms):
    """
    Build Aho-Corasick automaton from glossary terms
    Returns: automaton object
    """

def search_line_for_glossary(line, automaton):
    """
    Search a single line for glossary terms using Aho-Corasick
    Returns: list of matched terms
    """

def process_excel_lines(excel_path, automaton):
    """
    Read Excel, search each line, return results
    Returns: list of (original_line, glossary_terms_found) tuples
    """

def write_results_to_excel(results, output_path):
    """
    Write results to Excel with 2 columns:
    - Column 1: Original Line
    - Column 2: Glossary Terms Found (comma-separated)
    """

def main():
    """
    Main execution:
    1. Pick XML glossary source
    2. Build glossary
    3. Pick Excel input file
    4. Search for glossary terms
    5. Save results to Excel
    """
```

---

## âœ… Testing Plan

### Test Case 1: Basic Glossary Extraction
**Input XML**:
```xml
<LocStr StrOrigin="Kliff" Str="Kliff"/>
<LocStr StrOrigin="Calphade" Str="Calphade"/>
<LocStr StrOrigin="This is a long sentence that should be filtered." Str="..."/>
<LocStr StrOrigin="Hello, world!" Str="..."/>
```

**Expected Glossary**: `["Kliff", "Calphade"]`

### Test Case 2: Multi-Word Terms
**Input XML**:
```xml
<LocStr StrOrigin="Duke Elenor" Str="Duke Elenor"/>
<LocStr StrOrigin="Lands of Gogogugu" Str="Lands of Gogogugu"/>
```

**Expected Glossary**: `["Duke Elenor", "Lands of Gogogugu"]`

### Test Case 3: Line Search
**Glossary**: `["Kliff", "Calphade", "Duke Elenor"]`
**Excel Line**: `"Kliff went to Calphade to meet Duke Elenor"`
**Expected Match**: `"Kliff, Calphade, Duke Elenor"`

### Test Case 4: Overlapping Terms
**Glossary**: `["Duke", "Duke Elenor"]`
**Excel Line**: `"I am Duke Elenor"`
**Expected Match**: `"Duke Elenor"` (prefer longest match)

### Test Case 5: No Matches
**Glossary**: `["Kliff", "Calphade"]`
**Excel Line**: `"Hello world"`
**Expected Match**: `""` (empty)

---

## ğŸš€ Performance Goals

- **Glossary Building**: <5 seconds for 10,000 XML entries
- **Aho-Corasick Automaton**: <1 second for 5,000 terms
- **Line Searching**: <10 seconds for 10,000 lines with 5,000-term glossary
- **Excel Output**: <5 seconds for 10,000 rows

**Total**: <30 seconds for typical use case

---

## ğŸ“Š Success Criteria

âœ… **Functional**:
- Correctly extracts glossary from XML StrOrigin
- **5 HARDCODED RULES** applied automatically (no user config needed):
  1. Length < 15 chars âœ…
  2. Min 2 occurrences âœ…
  3. No punctuation endings âœ…
  4. No punctuation inside (except space/hyphen) âœ…
  5. Non-empty only âœ…
- **Word boundaries** enforced (no matching inside other words) âœ…
- Finds single-word Korean terms ("í´ë¦¬í”„") âœ…
- Finds multi-word Korean expressions ("ì—˜ë ˆë…¸ì–´ ê³µì‘") âœ…
- Handles overlapping matches correctly (prefers longest) âœ…
- Outputs clean Excel results âœ…

âœ… **Performance**:
- Fast enough for 10,000+ lines
- Uses Aho-Corasick for O(n+m+z) efficiency
- Two-pass filtering (like QuickSearch0818)

âœ… **User Experience**:
- Simple file picker GUI (no configuration needed!)
- Clear progress messages
- Helpful error messages
- Clean Excel output format
- Shows sample glossary terms after extraction

âœ… **Code Quality**:
- Clean, commented code
- Hardcoded rules well-documented
- Error handling
- Follows NewScripts patterns
- Based on proven QuickSearch0818 logic (lines 2113-2259)

---

## ğŸ”— References

### Pattern Source
**QuickSearch0818.py** (`RessourcesForCodingTheProject/SECONDARY PYTHON SCRIPTS/`)
- Lines 2113-2149: `glossary_filter()` function (filtering logic)
- Lines 2211-2259: Aho-Corasick automaton building and searching
- Line 24: `import ahocorasick`
- Line 890, 1775, 1803, 1879: `StrOrigin` extraction patterns

### Key Patterns Reused
1. **XML Parsing**: `tree.xpath('//LocStr')` with `locstr.get('StrOrigin', '')`
2. **Glossary Filtering**: Length threshold, punctuation removal, sentence detection
3. **Aho-Corasick**: Build automaton â†’ make_automaton() â†’ scan text for matches
4. **Excel I/O**: openpyxl patterns from multiple scripts

---

## ğŸ”„ ENHANCEMENT: Add Translation Mapping (Column 3)

**Status**: ğŸ“‹ PLANNED (2025-11-24)
**Complexity**: LOW (straightforward mapping)
**Estimated Time**: 30-45 minutes

### What Changes

**Current Output** (2 columns):
```
| Original Line | Glossary Terms Found |
```

**New Output** (3 columns):
```
| Original Line | Glossary Terms Found (StrOrigin) | Mapped Translations (Str) |
```

### Implementation Plan

#### Step 1: Update Glossary Extraction (extract_glossary_from_xml)
**Current**: Returns `list` of StrOrigin values only
```python
glossary = ['í´ë¦¬í”„', 'ì¹¼íŒŒë°', 'ì—˜ë ˆë…¸ì–´ ê³µì‘']  # Korean only
```

**New**: Return BOTH list (for Aho-Corasick) AND mapping dict
```python
glossary_terms = ['í´ë¦¬í”„', 'ì¹¼íŒŒë°', 'ì—˜ë ˆë…¸ì–´ ê³µì‘']  # Korean (for Aho-Corasick)
glossary_map = {
    'í´ë¦¬í”„': 'Kliff',              # Korean â†’ English
    'ì¹¼íŒŒë°': 'Calphade',           # Korean â†’ English
    'ì—˜ë ˆë…¸ì–´ ê³µì‘': 'Duke Elenor'   # Korean â†’ English
}
```

**Code changes**:
```python
def extract_glossary_from_xml(xml_path, length_threshold, min_occurrence):
    """
    Returns:
        tuple: (glossary_terms: list, glossary_map: dict)
        - glossary_terms: List of StrOrigin values for Aho-Corasick
        - glossary_map: Dict mapping StrOrigin â†’ Str values
    """
    # Extract BOTH StrOrigin and Str
    all_terms = []
    term_to_str_map = {}  # NEW: Store mapping

    for locstr in tree.xpath('//LocStr'):
        str_origin = locstr.get('StrOrigin', '').strip()
        str_value = locstr.get('Str', '').strip()

        if str_origin:
            all_terms.append(str_origin)
            term_to_str_map[str_origin] = str_value  # NEW: Map StrOrigin â†’ Str

    # Filter glossary (same as before)
    glossary_terms = filter_glossary_terms(all_terms, length_threshold, min_occurrence)

    # Build final mapping (only for terms that passed filtering)
    glossary_map = {term: term_to_str_map[term] for term in glossary_terms}

    return glossary_terms, glossary_map  # NEW: Return both
```

#### Step 2: Update Main Function
**Pass glossary_map through the pipeline**:
```python
def main():
    # Step 1: Extract glossary + mapping
    glossary_terms, glossary_map = extract_glossary_from_xml(xml_path)  # NEW: unpack tuple

    # Step 2: Build Aho-Corasick (uses glossary_terms only)
    automaton = build_ahocorasick_automaton(glossary_terms)

    # Step 3: Process Excel (pass glossary_map)
    results = process_excel_lines(excel_path, automaton, glossary_map)  # NEW: pass map

    # Step 4: Write results (now includes translations)
    write_results_to_excel(results, output_path)
```

#### Step 3: Update Excel Processing (process_excel_lines)
**Add glossary_map parameter**:
```python
def process_excel_lines(excel_path, automaton, glossary_map):  # NEW: glossary_map param
    """
    Returns:
        list: Tuples of (original_line, glossary_terms_found, mapped_translations)
    """
    for row in ws.iter_rows(...):
        line = str(row[0])
        matches = search_line_for_glossary(line, automaton)
        matches = resolve_overlapping_matches(matches, line)

        # NEW: Map each match to its Str value
        mapped_translations = [glossary_map.get(term, '') for term in matches]

        results.append((line, matches, mapped_translations))  # NEW: 3-tuple

    return results
```

#### Step 4: Update Excel Output (write_results_to_excel)
**Add third column**:
```python
def write_results_to_excel(results, output_path):
    # Header row (3 columns now)
    ws.append(["Original Line", "Glossary Terms Found (StrOrigin)", "Mapped Translations (Str)"])

    # Data rows
    for line, matches, translations in results:  # NEW: unpack 3-tuple
        glossary_str = ", ".join(matches) if matches else ""
        translation_str = ", ".join(translations) if translations else ""  # NEW

        ws.append([line, glossary_str, translation_str])  # NEW: 3 columns

    # Auto-size columns
    ws.column_dimensions['A'].width = 80
    ws.column_dimensions['B'].width = 50
    ws.column_dimensions['C'].width = 50  # NEW
```

### Testing Plan

**Test Case 1**: Basic mapping (Korean â†’ English)
- Input XML: `<LocStr StrOrigin="í´ë¦¬í”„" Str="Kliff"/>`
- Input line: "í´ë¦¬í”„ê°€ ë„ì‹œì— ê°”ë‹¤" (Korean)
- Expected output:
  - Column 2: "í´ë¦¬í”„"
  - Column 3: "Kliff"

**Test Case 2**: Multi-word expressions (Korean â†’ English)
- Input XML: `<LocStr StrOrigin="ì—˜ë ˆë…¸ì–´ ê³µì‘" Str="Duke Elenor"/>`
- Input line: "ë‚˜ëŠ” ì—˜ë ˆë…¸ì–´ ê³µì‘ì´ë‹¤" (Korean)
- Expected output:
  - Column 2: "ì—˜ë ˆë…¸ì–´ ê³µì‘"
  - Column 3: "Duke Elenor"

**Test Case 3**: Multiple matches (Korean â†’ English)
- Input line: "í´ë¦¬í”„ê°€ ì¹¼íŒŒë°ì— ê°”ë‹¤" (Korean)
- Expected output:
  - Column 2: "í´ë¦¬í”„, ì¹¼íŒŒë°"
  - Column 3: "Kliff, Calphade"

**Test Case 4**: No matches
- Input line: "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„" (Korean, no glossary terms)
- Expected output:
  - Column 2: ""
  - Column 3: ""

### Files to Modify

âœ… **glossary_sniffer_1124.py** (4 functions):
1. `extract_glossary_from_xml()` - Return tuple (list, dict)
2. `process_excel_lines()` - Add glossary_map param, return 3-tuple
3. `write_results_to_excel()` - Add 3rd column
4. `main()` - Update to pass glossary_map

âœ… **ROADMAP.md** (this file) - Document enhancement

âœ… **README.md** - Update example output (3 columns)

âœ… **SUMMARY.md** - Update example output (3 columns)

### Summary

**Changes**: Minimal, straightforward mapping
**Complexity**: LOW (just passing data through)
**Backward compatibility**: Output format changes (2 cols â†’ 3 cols)
**Testing**: 4 test cases to validate mapping
**Status**: âœ… COMPLETE (2025-11-24)

---

## ğŸŒ ENHANCEMENT: Multi-Language Support (13 Languages)

**Status**: ğŸ“‹ PLANNED (2025-11-24)
**Complexity**: MEDIUM (folder walking, multiple file parsing)
**Estimated Time**: 2-3 hours
**Reference**: wordcount1.py (lines 43-47, 134-148)

### Current Limitation

**Current**: Single XML file â†’ Single language mapping (Korean â†’ English)
```
Input: One XML file with StrOrigin + Str
Output: 3 columns (Original Line | Korean | English)
```

**Problem**: Need to check translations across **13 languages**, not just one!

### New Approach: Language Data Folder

**Input**: LANGUAGE DATA FOLDER containing multiple XML files
```
languagedata_folder/
â”œâ”€â”€ languagedata_KOR.xml  â† StrOrigin (Korean source)
â”œâ”€â”€ languagedata_ENG.xml  â† English translation
â”œâ”€â”€ languagedata_FRA.xml  â† French translation
â”œâ”€â”€ languagedata_GER.xml  â† German translation
â”œâ”€â”€ languagedata_SPA.xml  â† Spanish translation
â”œâ”€â”€ languagedata_ITA.xml  â† Italian translation
â”œâ”€â”€ languagedata_POR.xml  â† Portuguese translation
â”œâ”€â”€ languagedata_RUS.xml  â† Russian translation
â”œâ”€â”€ languagedata_POL.xml  â† Polish translation
â”œâ”€â”€ languagedata_TUR.xml  â† Turkish translation
â”œâ”€â”€ languagedata_THA.xml  â† Thai translation
â”œâ”€â”€ languagedata_JPN.xml  â† Japanese translation
â”œâ”€â”€ languagedata_CHS.xml  â† Chinese Simplified translation
â””â”€â”€ languagedata_CHT.xml  â† Chinese Traditional translation
```

**Key Insight from wordcount1.py**:
- All language files have the **SAME StrOrigin** values
- Each file has **different Str translations**
- Extract StrOrigin from KOR file (reference)
- Map to all 13 language translations

### New Output Format

**Output**: Excel with **15 columns** (Original Line + StrOrigin + 13 languages)

```
| Original Line (Korean) | Glossary (StrOrigin=KOR) | ENG | FRA | GER | SPA | ITA | POR | RUS | POL | TUR | THA | JPN | CHS | CHT |
|------------------------|--------------------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| í´ë¦¬í”„ê°€ ì¹¼íŒŒë°ì— ê°”ë‹¤   | í´ë¦¬í”„, ì¹¼íŒŒë°            | Kliff, Calphade | Kliff, Calphade | Kliff, Calphade | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
```

### Implementation Plan

#### Step 1: Update XML Selection (UI)
**Change**: Select FOLDER instead of single XML file
```python
def main():
    # BEFORE: Select single XML file
    xml_path = filedialog.askopenfilename(title="Select XML Glossary Source", ...)

    # AFTER: Select LANGUAGE DATA FOLDER
    folder_path = filedialog.askdirectory(title="Select Language Data Folder")
```

#### Step 2: Folder Walking (from wordcount1.py pattern)
**New function**: Walk folder and find all `languagedata_*.xml` files
```python
def iter_language_files(folder: Path):
    """
    Walk folder and yield all languagedata_*.xml files.
    Pattern from wordcount1.py lines 43-47.
    """
    for dirpath, _, filenames in os.walk(folder):
        for fn in filenames:
            if fn.lower().startswith("languagedata_") and fn.lower().endswith(".xml"):
                yield Path(dirpath) / fn

def extract_language_code(xml_path: Path) -> str:
    """
    Extract language code from filename.
    Example: "languagedata_ENG.xml" â†’ "ENG"
    Pattern from wordcount1.py lines 135-139.
    """
    stem = xml_path.stem
    parts = stem.split("_", 1)
    if len(parts) == 2:
        return parts[1].upper()
    return "UNKNOWN"
```

#### Step 3: Multi-Language Glossary Extraction
**New function**: Extract StrOrigin + all language translations
```python
def extract_multilanguage_glossary(folder_path, length_threshold, min_occurrence):
    """
    Extract glossary from LANGUAGE DATA FOLDER.

    Returns:
        tuple: (glossary_terms: list, multi_lang_map: dict)
        - glossary_terms: List of StrOrigin values (from KOR)
        - multi_lang_map: Dict mapping StrOrigin â†’ {lang_code: translation}

    Example multi_lang_map:
    {
        'í´ë¦¬í”„': {
            'ENG': 'Kliff',
            'FRA': 'Kliff',
            'GER': 'Kliff',
            'SPA': 'Kliff',
            ...
        },
        'ì¹¼íŒŒë°': {
            'ENG': 'Calphade',
            'FRA': 'Calphade',
            ...
        }
    }
    """
    # 1. Find all languagedata_*.xml files
    xml_files = list(iter_language_files(Path(folder_path)))
    print(f"   Found {len(xml_files)} language files")

    # 2. Group by language code
    files_by_lang = {}
    for xml_path in xml_files:
        lang_code = extract_language_code(xml_path)
        files_by_lang[lang_code] = xml_path

    # 3. Extract StrOrigin from KOR file (reference)
    if 'KOR' not in files_by_lang:
        raise ValueError("No Korean (KOR) language file found!")

    kor_path = files_by_lang['KOR']
    tree = etree.parse(kor_path)

    # Build StrOrigin list (for filtering)
    all_terms = []
    for locstr in tree.xpath('//LocStr'):
        str_origin = locstr.get('StrOrigin', '').strip()
        if str_origin:
            all_terms.append(str_origin)

    # Filter glossary (same rules as before)
    glossary_terms = filter_glossary_terms(all_terms, length_threshold, min_occurrence)

    # 4. Build multi-language mapping
    multi_lang_map = {term: {} for term in glossary_terms}

    for lang_code, xml_path in files_by_lang.items():
        print(f"   Processing {lang_code}...")
        tree = etree.parse(xml_path)

        for locstr in tree.xpath('//LocStr'):
            str_origin = locstr.get('StrOrigin', '').strip()
            str_value = locstr.get('Str', '').strip()

            # Only include terms that passed filtering
            if str_origin in multi_lang_map:
                multi_lang_map[str_origin][lang_code] = str_value

    return glossary_terms, multi_lang_map
```

#### Step 4: Update Excel Processing
**Modify**: Store translations for all languages
```python
def process_excel_lines(excel_path, automaton, multi_lang_map, language_codes):
    """
    Returns:
        list: Tuples of (original_line, glossary_terms_found, translations_by_lang)
        - translations_by_lang: Dict mapping lang_code â†’ comma-separated translations
    """
    results = []

    for row in ws.iter_rows(...):
        line = str(row[0])
        matches = search_line_for_glossary(line, automaton)
        matches = resolve_overlapping_matches(matches, line)

        # NEW: Build translations for ALL languages
        translations_by_lang = {}
        for lang_code in language_codes:
            lang_translations = [
                multi_lang_map.get(term, {}).get(lang_code, '')
                for term in matches
            ]
            translations_by_lang[lang_code] = lang_translations

        results.append((line, matches, translations_by_lang))

    return results
```

#### Step 5: Update Excel Output (15 Columns)
**New**: Write 15 columns instead of 3
```python
def write_results_to_excel(results, output_path, language_codes):
    """
    Write results with 15 columns:
    - Column 1: Original Line
    - Column 2: Glossary Terms Found (StrOrigin)
    - Columns 3-15: Translations for each language
    """
    # Header row
    headers = ["Original Line", "Glossary Terms (StrOrigin)"] + language_codes
    ws.append(headers)

    # Data rows
    for line, matches, translations_by_lang in results:
        glossary_str = ", ".join(matches) if matches else ""

        row_data = [line, glossary_str]

        # Add translation columns for each language
        for lang_code in language_codes:
            lang_translations = translations_by_lang.get(lang_code, [])
            translation_str = ", ".join(lang_translations) if lang_translations else ""
            row_data.append(translation_str)

        ws.append(row_data)

    # Auto-size columns
    ws.column_dimensions['A'].width = 80  # Original Line
    ws.column_dimensions['B'].width = 50  # Glossary Terms
    for i, lang_code in enumerate(language_codes, start=3):
        col_letter = chr(64 + i)  # C, D, E, F, ...
        ws.column_dimensions[col_letter].width = 40
```

### Testing Plan

**Test Case 1**: Folder with 13 language files
- Input: Folder with languagedata_KOR.xml, languagedata_ENG.xml, etc.
- Expected: Extracts glossary from KOR, maps to all 13 languages

**Test Case 2**: Multi-language mapping
- Input line: "í´ë¦¬í”„ê°€ ì¹¼íŒŒë°ì— ê°”ë‹¤"
- Expected output: 15 columns with translations in all languages

**Test Case 3**: Missing language file
- Input: Folder missing one language (e.g., no FRA file)
- Expected: Empty column for missing language, others work fine

**Test Case 4**: Same StrOrigin across files
- Verify: All language files have the same StrOrigin values
- Verify: Only Str values differ per language

### Files to Modify

âœ… **glossary_sniffer_1124.py** (major refactor):
1. Add `iter_language_files()` - Folder walking
2. Add `extract_language_code()` - Extract lang code from filename
3. Modify `extract_glossary_from_xml()` â†’ `extract_multilanguage_glossary()` - Multi-file parsing
4. Modify `process_excel_lines()` - Handle multi-language translations
5. Modify `write_results_to_excel()` - Write 15 columns

âœ… **ROADMAP.md** (this file) - Document enhancement

âœ… **README.md** - Update examples (15 columns)

âœ… **SUMMARY.md** - Update examples (15 columns)

### Summary

**Changes**: SIGNIFICANT refactor for multi-language support
**Complexity**: MEDIUM (folder walking, multiple file parsing, 15-column output)
**Backward compatibility**: BREAKS (output format changes 3 cols â†’ 15 cols)
**Testing**: 4 test cases + validation with real language data
**Benefits**:
- Check ALL 13 language translations at once âœ…
- Single run covers entire translation coverage âœ…
- No need to run 13 times for each language âœ…

---

## ğŸ“ Notes

- **Keep it standalone**: Single .py file, minimal dependencies
- **Speed matters**: Use Aho-Corasick for fast searching (proven in QuickSearch0818)
- **Smart filtering**: Don't include everything as glossary - filter intelligently
- **Multi-word support**: Must handle "Duke Elenor", "Lands of Gogogugu", etc.
- **User-friendly**: File pickers, progress messages, clear output

---

**Next Step**: Begin Phase 1 (XML Glossary Extraction) ğŸš€

---

*Last Updated: 2025-11-24*
*Status: Ready to build!*
