# Debug & Test Hub - Complete Remote Access Guide

**Priority:** Central Documentation | **Updated:** 2025-12-06

This is the **MASTER GUIDE** for all testing, debugging, and remote access methods.

---

## ðŸŽ¯ AUTONOMOUS TESTING PHILOSOPHY

**Claude works ALONE on testing. User provides direction only.**

```
TESTING PROTOCOL:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User Role:
â”œâ”€â”€ Overall design decisions
â”œâ”€â”€ Direction and priorities
â””â”€â”€ Final approval

Claude Role (FULLY AUTONOMOUS):
â”œâ”€â”€ Build the app
â”œâ”€â”€ Deploy to test folder
â”œâ”€â”€ Run ALL tests independently
â”œâ”€â”€ Fix issues found
â”œâ”€â”€ Rebuild and retest
â”œâ”€â”€ Report results
â””â”€â”€ NO USER INTERVENTION NEEDED
```

### ðŸ“‚ Windows Testing Playground

```
D:\LocaNext\              â† OFFICIAL WINDOWS TEST FOLDER
â”œâ”€â”€ LocaNext.exe          â† Built app
â”œâ”€â”€ server/               â† Backend
â”œâ”€â”€ logs/                 â† Test logs
â””â”€â”€ *.js                  â† CDP test scripts

WSL Access: /mnt/c/NEIL_PROJECTS_WINDOWSBUILD/LocaNextProject/LocaNext
```

### ðŸ“ Test Data Files (D:\TestFilesForLocaNext)

```
D:\TestFilesForLocaNext\          â† TEST DATA FOR ALL TOOLS
â”‚
â”œâ”€â”€ QuickSearch Test Files:
â”‚   â”œâ”€â”€ sampleofLanguageData.txt  â† RECOMMENDED (16MB, 9 cols, KR+FR)
â”‚   â””â”€â”€ SMALLTESTFILEFORQUICKSEARCH.txt â† âš ï¸ BAD (inconsistent columns)
â”‚
â”œâ”€â”€ XLSTransfer Test Files:
â”‚   â”œâ”€â”€ 150linetransaltiontest.xlsx
â”‚   â”œâ”€â”€ translationTEST.xlsx
â”‚   â”œâ”€â”€ TESTSMALL.xlsx
â”‚   â””â”€â”€ versysmallSMALLDB1.xlsx
â”‚
â”œâ”€â”€ KR Similar Test Files:
â”‚   â”œâ”€â”€ lineembeddingtest.xlsx
â”‚   â””â”€â”€ ê²€ì€ë³„test.xlsx
â”‚
â”œâ”€â”€ Glossary Test Files:
â”‚   â”œâ”€â”€ GlossaryUploadTestFile.xlsx
â”‚   â”œâ”€â”€ fileusedfordynamicglossary.xlsx
â”‚   â””â”€â”€ fileusedfornormalglossary.xlsx
â”‚
â””â”€â”€ Close Files:
    â”œâ”€â”€ closetotest.txt
    â””â”€â”€ closetotest_translated.txt

WSL Access: /mnt/d/TestFilesForLocaNext
```

**âš ï¸ IMPORTANT: QuickSearch requires 7+ column TSV files (cols 0-6):**
- Column 5 = Korean text
- Column 6 = Translation text
- Use `sampleofLanguageData.txt` NOT `SMALLTESTFILEFORQUICKSEARCH.txt`

**Claude has FULL authority to:**
- âœ… Erase everything and rebuild fresh
- âœ… Push new builds anytime
- âœ… Run CDP tests via remote debugging
- âœ… Auto-login (credentials in config)
- âœ… Modify code, test, iterate independently
- âœ… Install/uninstall as needed

**Claude does NOT need user for:**
- âŒ Running tests
- âŒ Starting/stopping app
- âŒ Reading logs
- âŒ Building new versions
- âŒ Deploying to test folder

---

## ðŸ”‘ DEV_MODE: Localhost Auto-Authentication

**For autonomous testing without manual login:**

```bash
# Start server with DEV_MODE
DEV_MODE=true python3 server/main.py
```

**What DEV_MODE does:**
- Auto-authenticates API calls from localhost (127.0.0.1, ::1)
- Returns `dev_admin` user with admin privileges
- No JWT token required for API testing
- Warning logged on startup: "DEV_MODE enabled"

**Security constraints:**
- ONLY works on localhost - remote requests still require auth
- Blocked if `PRODUCTION=true` is set
- If token provided, uses normal auth flow

**Use cases:**
- Claude autonomous API testing
- curl commands without auth header
- CDP tests without login flow
- Admin dashboard testing

**Files:**
- `server/config.py` - DEV_MODE flag
- `server/utils/dependencies.py` - auto-auth logic

---

## ðŸŒ MULTI-ENVIRONMENT TESTING (CRITICAL!)

### ðŸ“Š FULL MULTI-DIMENSIONAL TEST TREE

```
MULTI-DIMENSIONAL TESTING PROTOCOL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ ðŸ“ DIMENSION 1: Server Binding Check (FIRST!)
â”‚   â”‚
â”‚   â”œâ”€â”€ Command: netstat -tlnp 2>/dev/null | grep 8888
â”‚   â”‚
â”‚   â”œâ”€â”€ If 127.0.0.1:8888 â†’ ONLY WSL can access âŒ
â”‚   â”‚   â””â”€â”€ FIX: SERVER_HOST=0.0.0.0 python3 server/main.py
â”‚   â”‚
â”‚   â””â”€â”€ If 0.0.0.0:8888 â†’ All platforms can access âœ…
â”‚
â”œâ”€â”€ ðŸ“ DIMENSION 2: WSL Testing
â”‚   â”‚
â”‚   â”œâ”€â”€ curl tests
â”‚   â”‚   â”œâ”€â”€ curl -s http://localhost:8888/health
â”‚   â”‚   â””â”€â”€ curl -s http://localhost:5175/
â”‚   â”‚
â”‚   â””â”€â”€ API endpoints
â”‚       â”œâ”€â”€ curl http://localhost:8888/api/v2/admin/stats/database
â”‚       â””â”€â”€ curl http://localhost:8888/api/v2/admin/stats/server
â”‚
â”œâ”€â”€ ðŸ“ DIMENSION 3: Playwright Browser Simulation
â”‚   â”‚
â”‚   â”œâ”€â”€ Command: node scripts/visual-test.cjs --verbose
â”‚   â”‚
â”‚   â”œâ”€â”€ Checks:
â”‚   â”‚   â”œâ”€â”€ ALL console output (log, info, warn, error, debug)
â”‚   â”‚   â”œâ”€â”€ Screenshots to /tmp/dashboard_*.png
â”‚   â”‚   â”œâ”€â”€ API response codes
â”‚   â”‚   â”œâ”€â”€ DOM elements present
â”‚   â”‚   â””â”€â”€ "undefined" text detection
â”‚   â”‚
â”‚   â””â”€â”€ Output: 0 errors required, warnings should be fixed
â”‚
â”œâ”€â”€ ðŸ“ DIMENSION 4: Windows Browser Access
â”‚   â”‚
â”‚   â”œâ”€â”€ User opens: http://localhost:5175/database
â”‚   â”‚   â””â”€â”€ Should work if server bound to 0.0.0.0
â”‚   â”‚
â”‚   â””â”€â”€ Windows PowerShell test:
â”‚       â””â”€â”€ curl http://localhost:8888/health
â”‚
â”œâ”€â”€ ðŸ“ DIMENSION 5: LocaNext.exe (Electron)
â”‚   â”‚
â”‚   â”œâ”€â”€ Launch: ./LocaNext.exe --remote-debugging-port=9222
â”‚   â”œâ”€â”€ CDP endpoint: http://localhost:9222/json
â”‚   â””â”€â”€ Tests: node scripts/master_test.js
â”‚
â””â”€â”€ ðŸ“ DIMENSION 6: Cross-Platform Validation
    â”‚
    â”œâ”€â”€ Compare timestamps: When did user test vs when did you test?
    â”œâ”€â”€ Compare environments: Was server running for both?
    â””â”€â”€ Compare bindings: 127.0.0.1 vs 0.0.0.0?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Quick Multi-Dimensional Test Command

```bash
# Run ALL dimensions in sequence:

# D1: Check binding
netstat -tlnp 2>/dev/null | grep 8888

# D2: WSL curl
curl -s http://localhost:8888/health

# D3: Playwright
cd adminDashboard && node scripts/visual-test.cjs --verbose

# D4: Windows - user must verify (or check via PowerShell)
# D5: LocaNext - CDP tests if app running
# D6: Compare with user's environment
```

### âš ï¸ THE BIGGEST MISTAKE: Testing in ONE Environment Only

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  YOUR CURL FROM WSL â‰  USER'S WINDOWS BROWSER                                 â•‘
â•‘                                                                               â•‘
â•‘  If curl works in WSL but user sees ERR_CONNECTION_REFUSED:                  â•‘
â•‘  â†’ Server is bound to 127.0.0.1 (WSL only)                                   â•‘
â•‘  â†’ Windows browser can't reach WSL localhost                                 â•‘
â•‘                                                                               â•‘
â•‘  FIX: Start server with SERVER_HOST=0.0.0.0                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### The Environment Matrix

| Environment | What It Tests | How to Test |
|-------------|---------------|-------------|
| **WSL curl** | WSL localhost only | `curl http://localhost:8888/health` |
| **WSL Playwright** | WSL browser (headless) | `node scripts/visual-test.cjs` |
| **Windows Browser** | Windows â†’ WSL network | User opens `http://localhost:5175` |
| **LocaNext.exe** | Electron â†’ Backend | CDP tests via port 9222 |

### Server Binding Check (DO THIS FIRST!)

```bash
# Check what the server is bound to
netstat -tlnp 2>/dev/null | grep 8888

# If you see:
# 127.0.0.1:8888  â†’ ONLY WSL can access (Windows browser will FAIL!)
# 0.0.0.0:8888    â†’ ALL can access (Windows browser will work)
```

### Starting Server for Windows Access

```bash
# WRONG - Only WSL can access:
python3 server/main.py

# CORRECT - Windows browser can access:
SERVER_HOST=0.0.0.0 python3 server/main.py
```

### Full Multi-Environment Test Protocol

```bash
# === STEP 1: Check Server Binding ===
netstat -tlnp 2>/dev/null | grep 8888
# If 127.0.0.1, restart with SERVER_HOST=0.0.0.0

# === STEP 2: Test from WSL ===
curl -s http://localhost:8888/health
# Should return {"status":"healthy"...}

# === STEP 3: Test from WSL Playwright (simulates browser) ===
cd /home/neil1988/LocalizationTools/adminDashboard
node scripts/visual-test.cjs --verbose --page=/database

# === STEP 4: Check if Windows can reach it ===
# Windows PowerShell:
# curl http://localhost:8888/health
# OR user opens http://localhost:5175/database in browser
```

### Common Multi-Environment Issues

| Symptom | Environment Issue | Fix |
|---------|-------------------|-----|
| WSL curl works, Windows browser fails | Server bound to 127.0.0.1 | `SERVER_HOST=0.0.0.0` |
| Playwright works, user sees error | Different timing/state | Check when user tested vs when you tested |
| LocaNext.exe connects, browser fails | Different port/binding | Check all ports (8888, 5175, 5176) |
| Everything works locally, fails remotely | Firewall/network | Check Windows firewall |

### When User Reports Error

```
1. ASK: What environment? (Windows browser? LocaNext.exe? WSL?)
2. CHECK: Server binding (127.0.0.1 vs 0.0.0.0)
3. TEST: From SAME environment as user
4. NEVER: Assume your WSL curl represents their Windows browser
```

---

## ðŸš¨ CRITICAL: CLEANUP PROTOCOL (BEFORE EACH TEST RUN)

**ALWAYS kill test processes BEFORE launching new ones. Port conflicts = failures.**

### Kill LocaNext (Windows) - REQUIRED before each test:
```bash
# USE FULL PATH - tasklist.exe alone may fail silently!
/mnt/c/Windows/System32/taskkill.exe /F /IM "LocaNext.exe" /T

# Verify CLEAN
/mnt/c/Windows/System32/tasklist.exe | grep -i "loca" || echo "CLEAN"
```

### DO NOT KILL:
```
âŒ Gitea - needed for git push/commit
âŒ Other user processes
```

### Kill Backend (WSL):
```bash
fuser -k 8888/tcp 2>/dev/null || true
```

### Verification:
```bash
# Check all clear
/mnt/c/Windows/System32/tasklist.exe | grep -i loca   # Should be empty
curl -s http://localhost:8888/health                   # Should fail (no server)
curl -s http://localhost:3000/                         # Gitea should respond 200
```

### Known Hallucination Traps:
1. **`tasklist.exe` without full path may return empty** - ALWAYS use `/mnt/c/Windows/System32/tasklist.exe`
2. **"No process found" doesn't mean clean** - verify with full path
3. **Multiple LocaNext instances accumulate** - each test run adds more if not killed

---

## ðŸŽ¯ SINGLE-INSTANCE TESTING PROTOCOL (CRITICAL!)

**ROOT CAUSE OF MULTIPLE WINDOWS**: Each bash command with `./LocaNext.exe &` spawns a NEW window.

```
âš ï¸ NEVER DO THIS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Each of these spawns a NEW instance!
./LocaNext.exe &          # Instance 1
sleep 5
./LocaNext.exe &          # Instance 2 (BAD!)
curl ...
./LocaNext.exe &          # Instance 3 (WORSE!)
```

```
âœ… CORRECT APPROACH:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 1: Clean slate
â”œâ”€â”€ Kill ALL existing instances
â”œâ”€â”€ /mnt/c/Windows/System32/taskkill.exe /F /IM "LocaNext.exe" /T
â””â”€â”€ Verify: /mnt/c/Windows/System32/tasklist.exe | grep -i loca

STEP 2: Launch ONE instance
â”œâ”€â”€ cd /mnt/c/NEIL_PROJECTS_WINDOWSBUILD/LocaNextProject/LocaNext && ./LocaNext.exe --remote-debugging-port=9222 &
â”œâ”€â”€ Wait 40 seconds for full startup
â””â”€â”€ NEVER launch again until code changes!

STEP 3: Run ALL tests against that ONE instance
â”œâ”€â”€ Use curl/CDP/API tests - they don't spawn new windows
â”œâ”€â”€ Use pytest - runs against server, no new windows
â”œâ”€â”€ Use Node.js scripts - connects to existing CDP
â””â”€â”€ Reuse same instance for hours if needed

STEP 4: Only restart when:
â”œâ”€â”€ Code changes need testing
â”œâ”€â”€ App crashes or freezes
â”œâ”€â”€ Configuration changes
â””â”€â”€ NOT for each new test!
```

### Commands That SPAWN New Windows (AVOID!):
```bash
./LocaNext.exe &              # âŒ Spawns new window
/mnt/c/NEIL_PROJECTS_WINDOWSBUILD/LocaNextProject/LocaNext/LocaNext.exe  # âŒ Spawns new window
```

### Commands That DON'T Spawn Windows (SAFE):
```bash
curl http://localhost:8888/health           # âœ… API call only
curl http://localhost:9222/json             # âœ… CDP check only
node test_script.js                         # âœ… Connects to existing CDP
python3 -m pytest -v                        # âœ… Runs tests against server
/mnt/c/Windows/System32/tasklist.exe        # âœ… Just checks processes
```

### Testing Flow:
```
[INIT] Kill all â†’ Launch ONE â†’ Wait 40s
                      â†“
              [TEST LOOP]
              â†“        â†“
         curl tests   CDP tests   pytest
              â†“        â†“           â†“
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
              [REPEAT TESTS]
              (same instance)
                      â†“
         [ONLY RESTART IF CODE CHANGED]
```

---

## ðŸ§¹ SESSION START: BLOAT CHECK PROTOCOL (MANDATORY!)

**FIRST THING every new Claude session: Check for parasites from previous sessions.**

```bash
# === SESSION START BLOAT CHECK ===
# Run these BEFORE doing any work:

# 1. Check for Windows parasites (LocaNext.exe)
echo "=== WINDOWS PROCESSES ==="
/mnt/c/Windows/System32/tasklist.exe 2>/dev/null | grep -i "loca" || echo "CLEAN - No LocaNext"

# 2. Check for Linux port listeners
echo "=== PORT LISTENERS ==="
netstat -tlnp 2>/dev/null | grep -E ":(8888|5175|5176|3000)" || echo "CLEAN - No servers"

# 3. Check for stale Python servers
echo "=== PYTHON PROCESSES ==="
ps aux | grep -E "python.*main.py" | grep -v grep || echo "CLEAN - No Python servers"

# 4. Check for stale Vite/Node servers
echo "=== NODE PROCESSES ==="
ps aux | grep -E "node|vite" | grep -v grep | head -5 || echo "CLEAN - No Node servers"
```

### If Bloat Found - Clean It:

```bash
# Kill LocaNext parasites
/mnt/c/Windows/System32/taskkill.exe /F /IM "LocaNext.exe" /T 2>/dev/null || true

# Kill port listeners
fuser -k 8888/tcp 2>/dev/null || true  # Backend
fuser -k 5175/tcp 2>/dev/null || true  # Dashboard
fuser -k 5176/tcp 2>/dev/null || true  # LocaNext web

# Verify clean
/mnt/c/Windows/System32/tasklist.exe 2>/dev/null | grep -i "loca" || echo "CLEAN"
```

### Why This Matters:

| Symptom | Caused By |
|---------|-----------|
| Multiple windows appearing | Stale LocaNext.exe instances |
| "Port in use" errors | Previous server not killed |
| Tests pass but app broken | Testing against stale server |
| Inconsistent behavior | Multiple servers on same port |

---

## ðŸŽ­ PLAYWRIGHT: YOUR EYES INTO ANY BROWSER

### âš ï¸ CRITICAL CAPABILITY - READ THIS FIRST

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  YOU HAVE PLAYWRIGHT. YOU CAN CHECK WHAT ANY BROWSER SEES.                   â•‘
â•‘                                                                               â•‘
â•‘  âŒ FORBIDDEN PHRASES:                                                        â•‘
â•‘     - "I can't check what you see"                                           â•‘
â•‘     - "Tell me what you see when you load that page"                         â•‘
â•‘     - "The only way to know is for you to tell me"                           â•‘
â•‘     - "Can you send me a screenshot?"                                        â•‘
â•‘     - "What does your console show?"                                         â•‘
â•‘                                                                               â•‘
â•‘  âœ… WHAT TO DO INSTEAD:                                                       â•‘
â•‘     Run: node scripts/visual-test.cjs --verbose --page=/database             â•‘
â•‘     Or use Playwright directly to check the URL yourself!                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### How to Check What a Browser Sees (DO THIS!)

**Option 1: Use the visual-test.cjs script:**
```bash
cd /home/neil1988/LocalizationTools/adminDashboard
node scripts/visual-test.cjs --verbose --page=/database

# This shows you:
# - ALL console output (log, info, warn, error, debug)
# - Screenshots saved to /tmp/
# - API response status codes
# - DOM element verification
# - "undefined" text detection
```

**Option 2: Quick Playwright inline check:**
```bash
node -e "
const { chromium } = require('playwright');
(async () => {
  const browser = await chromium.launch({ headless: true });
  const page = await browser.newPage();

  // Capture ALL console output
  page.on('console', msg => console.log('[' + msg.type().toUpperCase() + ']', msg.text()));
  page.on('pageerror', err => console.log('[PAGE ERROR]', err.message));

  await page.goto('http://localhost:5175/database', { waitUntil: 'networkidle' });
  await page.screenshot({ path: '/tmp/check.png' });
  console.log('Screenshot saved to /tmp/check.png');

  await browser.close();
})();
"
```

**Option 3: Check specific elements:**
```bash
node -e "
const { chromium } = require('playwright');
(async () => {
  const browser = await chromium.launch({ headless: true });
  const page = await browser.newPage();
  await page.goto('http://localhost:5175/database');

  // Check for errors
  const hasError = await page.\$('.error-container');
  if (hasError) {
    const text = await hasError.textContent();
    console.log('ERROR FOUND:', text);
  } else {
    console.log('No error container visible');
  }

  // Check body for 'undefined'
  const body = await page.textContent('body');
  if (body.includes('undefined')) {
    console.log('WARNING: Found undefined in page content');
  }

  await browser.close();
})();
"
```

### When User Reports an Issue

```
USER: "The database page shows a 404 error"

WRONG RESPONSE:
"Can you tell me what the console shows?"
"I can't see what you're seeing"
"Please send a screenshot"

CORRECT RESPONSE:
*Runs Playwright to check http://localhost:5175/database*
"I checked the page with Playwright. Here's what I see: [results]"
```

---

## ðŸ” USER-VS-CLAUDE DISCREPANCY RESOLUTION PROTOCOL

**CRITICAL: When user reports issue X but your tests show Y, YOU ARE PROBABLY WRONG.**

```
âš ï¸ NEVER SAY: "It works for me" or "This is a caching issue"
âš ï¸ NEVER SAY: "Tell me what you see" - YOU HAVE PLAYWRIGHT, CHECK YOURSELF!
âš ï¸ NEVER: Start servers silently then claim "it works"
âš ï¸ NEVER: Test in YOUR environment while user is in THEIR environment
âœ… ALWAYS: Use Playwright to check URLs before asking user anything
âœ… ALWAYS: Trust user observation, investigate the difference
âœ… ALWAYS: Check what's running BEFORE starting anything new
âœ… ALWAYS: If you start a server, acknowledge it changes the environment
```

### The "I Started It" Trap

**WRONG:**
```
1. User reports: "Dashboard shows 404 error"
2. Claude runs: python3 server/main.py & (silently starts server)
3. Claude tests: curl http://localhost:8888 -> 200 OK
4. Claude says: "It works! No errors!"
5. User still sees 404 because THEIR browser loaded before Claude's server started
```

**RIGHT:**
```
1. User reports: "Dashboard shows 404 error"
2. Claude uses Playwright to check http://localhost:5175/database
3. Claude sees: "Error loading database stats - Not Found"
4. Claude checks: Is server running? NO
5. Claude acknowledges: "Playwright confirms the error. Server is NOT running."
```

### Step 1: Use Playwright FIRST

When user says "I see error X":
1. **USE PLAYWRIGHT** to check the exact URL they mentioned
2. See what Playwright shows - does it match user's report?
3. If it matches: You now see the issue, fix it
4. If it differs: Compare environments (server running? ports?)

```bash
# FIRST ACTION when user reports an issue:
node scripts/visual-test.cjs --verbose --page=/database
```

### Step 2: Compare Environments (only if needed)

| Check | User Environment | Claude Environment |
|-------|------------------|-------------------|
| Server running? | May have old server | May have just restarted |
| Time of test | When they reported | When you tested (could be later) |
| Browser cache | May have cached errors | Fresh Playwright session |
| Console output | ALL types (log, warn, error) | You may only capture errors |

### Step 3: Capture EVERYTHING

```javascript
// WRONG - Only captures errors:
page.on('console', msg => {
    if (msg.type() === 'error') { /* ... */ }  // Misses log, warn, info!
});

// CORRECT - Captures ALL console types:
page.on('console', msg => {
    console.log(`[${msg.type().toUpperCase()}] ${msg.text()}`);
});
```

### Common Discrepancy Causes:

| User Sees | Claude Tests Show | Root Cause |
|-----------|-------------------|------------|
| 404 error | 200 OK | Claude restarted server after user's test |
| Console errors | No errors | Claude only captures 'error' type, not all |
| "undefined" in UI | Data looks fine | Frontend/backend field mismatch |
| Broken page | Tests pass | Testing different URL or stale environment |

### Resolution Checklist:

- [ ] **Used Playwright to check the exact URL user mentioned**
- [ ] Captured ALL console types (log, info, warn, error, debug)
- [ ] Did NOT restart any servers between user report and my test
- [ ] Tested exact same URL user reported
- [ ] Screenshot saved and reviewed

---

## ðŸ–¼ï¸ ADMIN DASHBOARD VISUAL TESTING

**IMPORTANT**: Always run visual tests to catch display issues that automated tests miss.

### Prerequisites (CRITICAL!)

**The dashboard REQUIRES the backend server to be running:**

```bash
# STEP 1: Start backend server (port 8888)
cd /home/neil1988/LocalizationTools
python3 server/main.py &

# STEP 2: Start dashboard (port 5175)
cd adminDashboard
npm run dev -- --port 5175 &

# STEP 3: THEN run visual tests
node scripts/visual-test.cjs
```

**If backend is NOT running, you will see:**
```
GET http://localhost:8888/api/v2/admin/stats/database net::ERR_CONNECTION_REFUSED
API Request failed: TypeError: Failed to fetch
```

The visual-test.cjs script now checks for this and will exit with a clear error message if the backend is not running.

### Quick Visual Test Command

```bash
# Run from adminDashboard directory
cd /home/neil1988/LocalizationTools/adminDashboard
node scripts/visual-test.cjs
```

### What Visual Test Checks

| Page | Path | Key Checks |
|------|------|------------|
| Overview | `/` | Stats values, app rankings visible |
| Users | `/users` | User list loads |
| Stats | `/stats` | App/function rankings, no "undefined" |
| Telemetry | `/telemetry` | Page loads (data may be empty) |
| Logs | `/logs` | Log entries visible |
| Database | `/database` | DB stats, table list |
| Server | `/server` | CPU/Memory/Disk stats, system info |

### Screenshots Location

```
/tmp/dashboard_overview.png
/tmp/dashboard_users.png
/tmp/dashboard_stats.png
/tmp/dashboard_telemetry.png
/tmp/dashboard_logs.png
/tmp/dashboard_database.png
/tmp/dashboard_server.png
```

### Common Issues Detected

| Issue | Cause | Solution |
|-------|-------|----------|
| "undefined" in content | API field name mismatch | Check API returns vs frontend expects |
| 404 errors | Server needs restart | Restart `python3 server/main.py` |
| N/A values | Empty API response | Check API endpoint returns data |
| Error loading stats | Backend endpoint missing | Add endpoint to server |

### Manual Visual Verification

If visual test passes but you want to verify manually:

```bash
# 1. Start backend server
python3 server/main.py &

# 2. Start dashboard
cd adminDashboard && npm run dev -- --port 5175 &

# 3. Open in browser: http://localhost:5175
# 4. Navigate each page and check for errors
```

---

## ðŸ—ºï¸ CAPABILITIES TREE (What Claude Can Do)

```
REMOTE ACCESS & TESTING CAPABILITIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ ðŸ–¥ï¸ WINDOWS EXE DEBUGGING (from WSL)
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“¡ CDP (Chrome DevTools Protocol) â”€â”€â”€â”€ MOST POWERFUL
â”‚   â”‚   â”œâ”€â”€ Launch: LocaNext.exe --remote-debugging-port=9222
â”‚   â”‚   â”œâ”€â”€ Connect: http://localhost:9222/json
â”‚   â”‚   â”œâ”€â”€ Capabilities:
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Read DOM state
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Execute JavaScript
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Click buttons programmatically
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Fill forms
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Navigate pages
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Take screenshots
â”‚   â”‚   â”‚   â”œâ”€â”€ âœ… Monitor console logs
â”‚   â”‚   â”‚   â””â”€â”€ âœ… Intercept network requests
â”‚   â”‚   â”œâ”€â”€ Test Scripts: locaNext/scripts/master_test.js
â”‚   â”‚   â””â”€â”€ Doc: WINDOWS_TROUBLESHOOTING.md
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ Log File Access
â”‚   â”‚   â”œâ”€â”€ Path: /mnt/c/Users/.../AppData/Local/LocaNext/logs/
â”‚   â”‚   â”œâ”€â”€ Read: cat /mnt/c/.../main.log
â”‚   â”‚   â”œâ”€â”€ Watch: tail -f /mnt/c/.../main.log
â”‚   â”‚   â””â”€â”€ All Electron logs written here
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ”Œ Process Control
â”‚       â”œâ”€â”€ Launch: /mnt/c/.../LocaNext.exe &
â”‚       â”œâ”€â”€ Kill: taskkill.exe /IM LocaNext.exe /F
â”‚       â””â”€â”€ Check: tasklist.exe | grep LocaNext
â”‚
â”œâ”€â”€ ðŸ BACKEND TESTING (Python)
â”‚   â”‚
â”‚   â”œâ”€â”€ pytest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PRIMARY TEST RUNNER
â”‚   â”‚   â”œâ”€â”€ Quick: python3 -m pytest -v
â”‚   â”‚   â”œâ”€â”€ With Server: RUN_API_TESTS=1 python3 -m pytest -v
â”‚   â”‚   â”œâ”€â”€ Single file: python3 -m pytest tests/api/test_remote_logging.py -v
â”‚   â”‚   â”œâ”€â”€ Coverage: python3 -m pytest --cov=server
â”‚   â”‚   â””â”€â”€ Doc: PYTEST_GUIDE.md
â”‚   â”‚
â”‚   â”œâ”€â”€ Direct API Testing
â”‚   â”‚   â”œâ”€â”€ curl: curl -X GET http://localhost:8888/health
â”‚   â”‚   â”œâ”€â”€ Python requests: In test files
â”‚   â”‚   â””â”€â”€ httpie: http GET localhost:8888/health
â”‚   â”‚
â”‚   â””â”€â”€ Database Inspection (PostgreSQL)
â”‚       â”œâ”€â”€ Connect: psql -h 127.0.0.1 -p 5432 -U localization_admin -d localizationtools
â”‚       â”œâ”€â”€ Tables: \dt
â”‚       â””â”€â”€ Query: SELECT * FROM users;
â”‚
â”œâ”€â”€ ðŸŽ­ PLAYWRIGHT - YOUR EYES INTO ANY BROWSER (USE THIS!)
â”‚   â”‚
â”‚   â”œâ”€â”€ Playwright â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SEE WHAT ANY URL SHOWS
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ âš ï¸ FORBIDDEN: Saying "I can't check what you see"
â”‚   â”‚   â”œâ”€â”€ âš ï¸ FORBIDDEN: Saying "Tell me what the console shows"
â”‚   â”‚   â”œâ”€â”€ âœ… DO THIS: Use Playwright to check URLs yourself!
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Quick Check Any URL:
â”‚   â”‚   â”‚   â””â”€â”€ node scripts/visual-test.cjs --verbose --page=/database
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Admin Dashboard Visual Test:
â”‚   â”‚   â”‚   â”œâ”€â”€ cd adminDashboard && node scripts/visual-test.cjs
â”‚   â”‚   â”‚   â”œâ”€â”€ Screenshots: /tmp/dashboard_*.png
â”‚   â”‚   â”‚   â””â”€â”€ Shows ALL console output (log, info, warn, error)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Inline Playwright Check:
â”‚   â”‚   â”‚   â””â”€â”€ node -e "require('playwright').chromium.launch()..."
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ E2E Tests:
â”‚   â”‚   â”‚   â”œâ”€â”€ Run: cd locaNext && npm test
â”‚   â”‚   â”‚   â”œâ”€â”€ Headed: npm test -- --headed
â”‚   â”‚   â”‚   â””â”€â”€ Debug: npm test -- --debug
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Doc: PLAYWRIGHT_GUIDE.md
â”‚   â”‚
â”‚   â”œâ”€â”€ Visual Testing (X Server)
â”‚   â”‚   â”œâ”€â”€ Setup: Start VcXsrv on Windows
â”‚   â”‚   â”œâ”€â”€ Export: export DISPLAY=:0
â”‚   â”‚   â”œâ”€â”€ Test: xeyes (should show eyes window)
â”‚   â”‚   â””â”€â”€ Doc: X_SERVER_SETUP.md
â”‚   â”‚
â”‚   â””â”€â”€ Browser DevTools
â”‚       â”œâ”€â”€ Electron: Ctrl+Shift+I or DEBUG_MODE=true
â”‚       â”œâ”€â”€ Console: View JS errors
â”‚       â””â”€â”€ Network: Monitor API calls
â”‚
â”œâ”€â”€ ðŸ”„ REAL-TIME MONITORING
â”‚   â”‚
â”‚   â”œâ”€â”€ WebSocket
â”‚   â”‚   â”œâ”€â”€ Connect: ws://localhost:8888/ws/socket.io
â”‚   â”‚   â”œâ”€â”€ Events: progress, logs, task updates
â”‚   â”‚   â””â”€â”€ Test: wscat -c ws://localhost:8888/ws/socket.io
â”‚   â”‚
â”‚   â”œâ”€â”€ Server Logs
â”‚   â”‚   â”œâ”€â”€ Watch: tail -f /tmp/server.log
â”‚   â”‚   â”œâ”€â”€ Errors: grep ERROR /tmp/server.log
â”‚   â”‚   â””â”€â”€ Script: bash scripts/monitor_logs_realtime.sh
â”‚   â”‚
â”‚   â””â”€â”€ Health Endpoints
â”‚       â”œâ”€â”€ Main: http://localhost:8888/health
â”‚       â”œâ”€â”€ Telemetry: http://localhost:8888/api/v1/remote-logs/health
â”‚       â””â”€â”€ DB: Included in /health response
â”‚
â”œâ”€â”€ ðŸ“Š TELEMETRY TESTING (P12.5)
â”‚   â”‚
â”‚   â”œâ”€â”€ Two-Port Simulation
â”‚   â”‚   â”œâ”€â”€ Desktop: python3 server/main.py (port 8888)
â”‚   â”‚   â”œâ”€â”€ Central: SERVER_PORT=9999 python3 server/main.py
â”‚   â”‚   â””â”€â”€ Test: RUN_API_TESTS=1 python3 -m pytest tests/api/test_remote_logging.py -v
â”‚   â”‚
â”‚   â””â”€â”€ Telemetry API Endpoints
â”‚       â”œâ”€â”€ POST /api/v1/remote-logs/register
â”‚       â”œâ”€â”€ POST /api/v1/remote-logs/sessions/start
â”‚       â”œâ”€â”€ POST /api/v1/remote-logs/submit
â”‚       â””â”€â”€ POST /api/v1/remote-logs/sessions/end
â”‚
â””â”€â”€ ðŸ› ï¸ UTILITY TOOLS
    â”‚
    â”œâ”€â”€ Screenshots/Recording
    â”‚   â”œâ”€â”€ scrot: scrot screenshot.png (X Server needed)
    â”‚   â”œâ”€â”€ ffmpeg: Record video
    â”‚   â””â”€â”€ Doc: TOOLS_REFERENCE.md
    â”‚
    â”œâ”€â”€ UI Automation (Linux)
    â”‚   â”œâ”€â”€ xdotool: xdotool search --name "LocaNext"
    â”‚   â”œâ”€â”€ Click: xdotool click 1
    â”‚   â””â”€â”€ Type: xdotool type "hello"
    â”‚
    â””â”€â”€ Network Tools
        â”œâ”€â”€ netstat: netstat -tlnp | grep 8888
        â”œâ”€â”€ curl: API testing
        â””â”€â”€ wget: Download files

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ðŸ“š DOCUMENTATION TREE

```
docs/testing/
â”‚
â”œâ”€â”€ DEBUG_AND_TEST_HUB.md â”€â”€â”€â”€ THIS FILE (Master Guide)
â”‚
â”œâ”€â”€ ðŸš€ Quick Start
â”‚   â””â”€â”€ QUICK_COMMANDS.md â”€â”€â”€â”€ Copy-paste commands
â”‚
â”œâ”€â”€ ðŸ Backend Testing
â”‚   â””â”€â”€ PYTEST_GUIDE.md â”€â”€â”€â”€â”€â”€ pytest patterns, fixtures
â”‚
â”œâ”€â”€ ðŸŒ Frontend Testing
â”‚   â”œâ”€â”€ PLAYWRIGHT_GUIDE.md â”€â”€ E2E browser tests
â”‚   â””â”€â”€ X_SERVER_SETUP.md â”€â”€â”€â”€ Visual testing from WSL
â”‚
â”œâ”€â”€ ðŸ–¥ï¸ Windows/Electron
â”‚   â””â”€â”€ ../WINDOWS_TROUBLESHOOTING.md â”€â”€ CDP, logs, debugging
â”‚
â”œâ”€â”€ ðŸ“Š Admin Dashboard
â”‚   â””â”€â”€ scripts/visual-test.cjs â”€â”€ Visual testing script (7 pages)
â”‚
â””â”€â”€ ðŸ› ï¸ Tools
    â””â”€â”€ TOOLS_REFERENCE.md â”€â”€â”€â”€ xdotool, ffmpeg, scrot
```

**Related Docs (outside testing/):**
- `docs/ELECTRON_TROUBLESHOOTING.md` - Black screen, preload issues
- `docs/MONITORING_COMPLETE_GUIDE.md` - Log monitoring
- `docs/LOGGING_PROTOCOL.md` - How to log properly

---

## âš¡ QUICK REFERENCE

### Start Everything for Full Testing

```bash
# Terminal 1: Backend
python3 server/main.py > /tmp/server.log 2>&1 &

# Terminal 2: Watch logs
tail -f /tmp/server.log

# Terminal 3: Run tests
RUN_API_TESTS=1 python3 -m pytest -v
```

### Test Windows EXE via CDP

```bash
# 1. Launch with debugging (from WSL)
/mnt/c/Users/.../LocaNext/LocaNext.exe --remote-debugging-port=9222 &

# 2. Wait for startup
sleep 30

# 3. Check pages available
curl -s http://localhost:9222/json | jq '.[].url'

# 4. Run CDP tests
node /mnt/c/.../locaNext/scripts/master_test.js
```

### Test Telemetry API

```bash
# Register
curl -X POST http://localhost:8888/api/v1/remote-logs/register \
  -H "Content-Type: application/json" \
  -d '{"installation_name": "Test", "version": "1.0.0"}'

# Submit logs (use api_key from register response)
curl -X POST http://localhost:8888/api/v1/remote-logs/submit \
  -H "Content-Type: application/json" \
  -H "x-api-key: YOUR_API_KEY" \
  -d '{"installation_id": "YOUR_ID", "logs": [{"timestamp": "2025-12-05T12:00:00Z", "level": "INFO", "message": "test", "source": "test", "installation_id": "YOUR_ID"}]}'
```

---

## ðŸ§ª TEST COUNTS

| Category | Tests | Tool | Doc |
|----------|-------|------|-----|
| Backend Unit | 350+ | pytest | PYTEST_GUIDE.md |
| Backend E2E | 115 | pytest | PYTEST_GUIDE.md |
| API Simulation | 168 | pytest | PYTEST_GUIDE.md |
| Security | 86 | pytest | SECURITY_HARDENING.md |
| Telemetry | 10 | pytest | test_remote_logging.py |
| Frontend LocaNext | 134 | Playwright | PLAYWRIGHT_GUIDE.md |
| Frontend Dashboard | 30 | Playwright | PLAYWRIGHT_GUIDE.md |
| **Dashboard Visual** | **7 pages** | **Node.js/Playwright** | **scripts/visual-test.cjs** |
| CDP (Windows EXE) | 15 | Node.js | WINDOWS_TROUBLESHOOTING.md |
| **Total** | **~1000+** | | |

---

## ðŸ”‘ KEY PATTERNS

### CDP: Finding the App Page (Not DevTools)

```javascript
// CDP returns multiple pages - find the actual app
const pages = await fetch('http://localhost:9222/json').then(r => r.json());
const appPage = pages.find(p =>
    p.type === 'page' &&
    p.url.includes('file:') &&
    !p.url.includes('devtools')
);
```

### pytest: TRUE Simulation (No Mocks)

```python
# Skip if server not running
pytestmark = pytest.mark.skipif(
    not os.getenv("RUN_API_TESTS"),
    reason="RUN_API_TESTS not set"
)

# Use real API client
class APIClient:
    def __init__(self, base_url="http://127.0.0.1:8888"):
        self.session = requests.Session()
```

### Playwright: Headless by Default

```typescript
// In playwright.config.ts
use: {
    headless: true,  // Set false for visual debugging
    screenshot: 'only-on-failure',
}
```

---

## ðŸ“‹ TROUBLESHOOTING QUICK FIXES

| Problem | Solution |
|---------|----------|
| Port 8888 in use | `fuser -k 8888/tcp` |
| CDP can't connect | Check if app started, wait longer |
| X Server not working | `export DISPLAY=:0` |
| pytest skipping tests | Set `RUN_API_TESTS=1` |
| Electron black screen | Check preload.js errors |
| WebSocket not connecting | Verify server started |

---

*Last updated: 2025-12-06 - MULTI-ENVIRONMENT TESTING: WSL curl â‰  Windows browser! Check server binding (127.0.0.1 vs 0.0.0.0). PLAYWRIGHT: Never say "tell me what you see" - check yourself!*
