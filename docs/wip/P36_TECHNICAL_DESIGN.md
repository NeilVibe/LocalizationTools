# P36: Technical Design Document

**Priority:** P36 | **Status:** DESIGN PHASE | **Created:** 2025-12-17

---

## Purpose

This document covers the **foundational design decisions** for P36 Pretranslation System before any code is written. These decisions affect architecture, performance, and data integrity.

---

## Table of Contents

0. [Database Changes Required](#0-database-changes-required) â† **NEW**
1. [Batch Processing Architecture](#1-batch-processing-architecture)
2. [StringID Handling in Embeddings](#2-stringid-handling-in-embeddings)
3. [Glossary Creation Flow](#3-glossary-creation-flow)
4. [Data Preprocessing Strategy](#4-data-preprocessing-strategy)

---

## 0. Database Changes Required

**Reviewed:** 2025-12-17 | **Status:** â³ TO IMPLEMENT

### Current DB Model Analysis

#### LDMRow (for Files) - âœ… HAS StringID
```python
# server/database/models.py:636-678
class LDMRow(Base):
    __tablename__ = "ldm_rows"
    string_id = Column(String(255), nullable=True, index=True)  # âœ… EXISTS
    source = Column(Text, nullable=True)
    target = Column(Text, nullable=True)
```

#### LDMTMEntry (for TM) - âŒ MISSING StringID
```python
# server/database/models.py:795-831
class LDMTMEntry(Base):
    __tablename__ = "ldm_tm_entries"
    source_text = Column(Text, nullable=False)
    target_text = Column(Text, nullable=True)
    source_hash = Column(String(64), nullable=False, index=True)
    # âŒ NO string_id column!
```

#### LDMTranslationMemory - âŒ MISSING Mode
```python
# server/database/models.py:747-792
class LDMTranslationMemory(Base):
    __tablename__ = "ldm_translation_memories"
    name = Column(String(255), nullable=False)
    source_lang = Column(String(10), default="ko")
    target_lang = Column(String(10), default="en")
    # âŒ NO mode column (standard vs stringid)!
```

### Required Changes

#### 1. Add `string_id` to `LDMTMEntry`
```python
# ADD to server/database/models.py LDMTMEntry class
string_id = Column(String(255), nullable=True, index=True)
```

#### 2. Add `mode` to `LDMTranslationMemory`
```python
# ADD to server/database/models.py LDMTranslationMemory class
mode = Column(String(20), default="standard")  # "standard" or "stringid"
```

#### 3. Update Indexes
```python
# In LDMTMEntry.__table_args__
Index("idx_ldm_tm_entry_stringid", "string_id"),
Index("idx_ldm_tm_entry_tm_hash_stringid", "tm_id", "source_hash", "string_id"),
```

#### 4. Migration Script
```sql
-- Alembic migration or manual SQL
ALTER TABLE ldm_tm_entries ADD COLUMN string_id VARCHAR(255);
CREATE INDEX idx_ldm_tm_entry_stringid ON ldm_tm_entries(string_id);

ALTER TABLE ldm_translation_memories ADD COLUMN mode VARCHAR(20) DEFAULT 'standard';
```

### Excel Handler Update

#### Current (`tm_manager.py:193-241`)
```python
def _parse_excel_for_tm(self, ..., source_col=0, target_col=1):
    # Only 2 columns - NO StringID support
```

#### Required
```python
def _parse_excel_for_tm(self, ..., source_col=0, target_col=1, stringid_col=None, mode="standard"):
    # Support optional StringID column
    # Validate based on mode (strict for stringid, lenient for standard)
```

### Implementation Order

1. âœ… DB model changes (add columns)
2. âœ… Migration script
3. âœ… Excel handler update
4. âœ… API updates (accept mode + stringid_column params)
5. âœ… PKL builder update (variations structure)
6. âœ… Frontend modal (mode selection + column mapping)

---

## 1. Batch Processing Architecture

### Resource Usage: 100% LOCAL Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRETRANSLATION = LOCAL CPU POWER                          â”‚
â”‚                                                                             â”‚
â”‚  USER'S PC (LocaNext.exe)          CENTRAL SERVER (PostgreSQL)             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â”‚
â”‚  âœ… Qwen embeddings (2.3GB)        âŒ No compute                            â”‚
â”‚  âœ… FAISS vector search            âœ… Data storage only                     â”‚
â”‚  âœ… Hash lookups                   âœ… User auth                             â”‚
â”‚  âœ… N-gram matching                âœ… Session management                    â”‚
â”‚  âœ… Multiprocessing (4 workers)                                             â”‚
â”‚  âœ… Celery + Redis (local)                                                  â”‚
â”‚                                                                             â”‚
â”‚  WHY LOCAL?                                                                 â”‚
â”‚  â”œâ”€â”€ No network latency for millions of vector comparisons                 â”‚
â”‚  â”œâ”€â”€ User's CPU is dedicated to their work                                 â”‚
â”‚  â”œâ”€â”€ Server doesn't bottleneck under multi-user load                       â”‚
â”‚  â””â”€â”€ Offline mode works (SQLite fallback)                                  â”‚
â”‚                                                                             â”‚
â”‚  CPU USAGE:                                                                 â”‚
â”‚  â”œâ”€â”€ Embedding generation: HIGH (Qwen model inference)                     â”‚
â”‚  â”œâ”€â”€ FAISS search: MEDIUM-HIGH (vector similarity)                         â”‚
â”‚  â”œâ”€â”€ Hash/N-gram: LOW                                                      â”‚
â”‚  â””â”€â”€ Workers: 4 (uses ~50% of CPU, leaves headroom)                        â”‚
â”‚                                                                             â”‚
â”‚  MEMORY USAGE:                                                              â”‚
â”‚  â”œâ”€â”€ Qwen model: ~2.3GB (loaded once)                                      â”‚
â”‚  â”œâ”€â”€ FAISS index: Varies by TM size (~100MB per 100k entries)              â”‚
â”‚  â””â”€â”€ Per chunk: ~50MB (500 rows Ã— embeddings)                              â”‚
â”‚                                                                             â”‚
â”‚  EXPECTED PERFORMANCE:                                                      â”‚
â”‚  â”œâ”€â”€ 10,000 rows: ~2 minutes                                               â”‚
â”‚  â”œâ”€â”€ 100,000 rows: ~20 minutes                                             â”‚
â”‚  â””â”€â”€ Scales linearly with row count                                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current Infrastructure

We already have Celery + Redis:

```python
# server/tasks/celery_app.py (EXISTS)
broker = redis://localhost:6379/1
result_backend = redis://localhost:6379/2
task_time_limit = 3600      # 1 hour hard limit
task_soft_time_limit = 3000  # 50 min soft limit
```

### The Question: How to Optimize?

| Option | Description | Pros | Cons |
|--------|-------------|------|------|
| **A. Celery Only** | Single queue, single worker | Simple, already exists | Slower for large batches |
| **B. Celery + Multiprocess** | Queue + multiprocessing within worker | Faster, uses all CPU cores | More complex, memory usage |
| **C. Multiple Celery Workers** | Multiple worker processes | Scalable, distributed | More infrastructure |
| **D. Chunked Processing** | Break into chunks, progress tracking | Resumable, good UX | Overhead per chunk |

### RECOMMENDATION: Option D (Chunked) + Option B (Multiprocess)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECOMMENDED ARCHITECTURE                                  â”‚
â”‚                                                                             â”‚
â”‚  User Request: Pretranslate 10,000 rows                                    â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ CELERY TASK: pretranslate_batch                                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  1. Validate request                                                â”‚   â”‚
â”‚  â”‚  2. Split into chunks (500 rows each = 20 chunks)                  â”‚   â”‚
â”‚  â”‚  3. For each chunk:                                                 â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ Multiprocess pool (4-8 workers)                            â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ Each worker processes ~60-125 rows                         â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ Update progress via WebSocket                              â”‚   â”‚
â”‚  â”‚     â””â”€â”€ Save results to DB                                          â”‚   â”‚
â”‚  â”‚  4. Return final results                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Benefits:                                                                  â”‚
â”‚  - Resumable (if task fails, restart from last chunk)                      â”‚
â”‚  - Progress tracking (user sees 5/20 chunks done)                          â”‚
â”‚  - Memory efficient (only 500 rows in memory at a time)                    â”‚
â”‚  - Fast (multiprocessing uses all CPU cores)                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Parameters

```python
# Recommended defaults (can be tuned)
CHUNK_SIZE = 500           # Rows per chunk
WORKERS_PER_CHUNK = 4      # Multiprocess workers (adjust based on CPU cores)
PROGRESS_UPDATE_INTERVAL = 50  # Update WebSocket every N rows
```

### Why This Works

| Metric | Value | Reasoning |
|--------|-------|-----------|
| **Chunk Size: 500** | ~500 rows | Small enough for memory, large enough for efficiency |
| **Workers: 4** | CPU cores / 2 | Leave CPU headroom for other tasks |
| **Progress: 50 rows** | ~10 updates per chunk | Responsive UX without WebSocket spam |

### Code Structure (Conceptual)

```python
# server/tasks/pretranslate_tasks.py

from celery import shared_task
from multiprocessing import Pool

@shared_task(bind=True)
def pretranslate_batch(self, file_id: int, engine: str, dictionary_id: int, threshold: float):
    """
    Main Celery task for batch pretranslation.
    Uses chunking + multiprocessing for optimal performance.
    """
    # 1. Load file rows
    rows = load_file_rows(file_id)
    total_rows = len(rows)

    # 2. Split into chunks
    chunks = list(chunked(rows, CHUNK_SIZE))
    total_chunks = len(chunks)

    results = []
    for chunk_idx, chunk in enumerate(chunks):
        # 3. Process chunk with multiprocessing
        with Pool(processes=WORKERS_PER_CHUNK) as pool:
            chunk_results = pool.map(
                partial(process_row, engine=engine, dictionary_id=dictionary_id, threshold=threshold),
                chunk
            )

        # 4. Save chunk results to DB
        save_chunk_results(file_id, chunk_results)

        # 5. Update progress
        progress = (chunk_idx + 1) / total_chunks * 100
        self.update_state(state='PROGRESS', meta={'progress': progress, 'chunk': chunk_idx + 1, 'total': total_chunks})
        send_websocket_progress(file_id, progress)

        results.extend(chunk_results)

    return {'status': 'complete', 'total_processed': len(results)}
```

### Open Questions

- [ ] Should we allow cancellation mid-batch?
- [ ] How to handle partial failures (some rows fail, others succeed)?
- [ ] Should chunks be processed in parallel (multiple Celery tasks)?

---

## 2. StringID Handling in Embeddings

### User Choice: Process WITH or WITHOUT StringID

**The user decides at TM creation time:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TM CREATION - MODE SELECTION                              â”‚
â”‚                                                                             â”‚
â”‚  â—‹ Standard Mode (Source + Target only)                                    â”‚
â”‚    â””â”€â”€ Rows without target are skipped                                     â”‚
â”‚    â””â”€â”€ Most common translation wins for duplicates                         â”‚
â”‚    â””â”€â”€ Works with any file format                                          â”‚
â”‚                                                                             â”‚
â”‚  â—‹ StringID Mode (Source + Target + StringID)                              â”‚
â”‚    â””â”€â”€ PRECHECK: All rows MUST have Source + Target + StringID             â”‚
â”‚    â””â”€â”€ If any row is missing data â†’ ERROR, cannot process                  â”‚
â”‚    â””â”€â”€ Preserves context differentiation for game UI                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### StringID Mode: Strict Validation

**If user selects StringID mode, we run a PRECHECK:**

```python
def precheck_stringid_mode(df: pd.DataFrame) -> tuple[bool, str, dict]:
    """
    Validate data for StringID mode processing.
    ALL rows must have Source + Target + StringID.

    Returns: (is_valid, error_message, stats)
    """
    stats = {
        'total_rows': len(df),
        'missing_source': 0,
        'missing_target': 0,
        'missing_stringid': 0,
        'valid_rows': 0
    }

    # Check for missing data
    stats['missing_source'] = df['source'].isna().sum() + (df['source'] == '').sum()
    stats['missing_target'] = df['target'].isna().sum() + (df['target'] == '').sum()
    stats['missing_stringid'] = df['string_id'].isna().sum() + (df['string_id'] == '').sum()
    stats['valid_rows'] = len(df) - max(stats['missing_source'], stats['missing_target'], stats['missing_stringid'])

    # If ANY discrepancies, reject
    if stats['missing_source'] > 0 or stats['missing_target'] > 0 or stats['missing_stringid'] > 0:
        error_msg = f"""
Data has discrepancies and cannot be processed with StringID mode:
- Rows missing Source: {stats['missing_source']}
- Rows missing Target: {stats['missing_target']}
- Rows missing StringID: {stats['missing_stringid']}
- Total rows: {stats['total_rows']}

Please clean your data or use Standard Mode instead.
"""
        return False, error_msg, stats

    return True, "", stats
```

### Standard Mode: Lenient Processing

**If user selects Standard mode (no StringID), use existing robust logic:**

```python
# ALREADY EXISTS in process_operation.py:85
df = df.iloc[:, [col_kr_index, col_fr_index]].dropna()  # Skip rows without target

# ALREADY EXISTS - clean_text() removes _x000D_, strips whitespace
kr_texts = df.iloc[:, 0].apply(clean_text).tolist()
fr_texts = df.iloc[:, 1].apply(clean_text).tolist()

# ALREADY EXISTS - most frequent translation wins for duplicates
most_freq_trans = df.groupby('KR')['FR'].agg(safe_most_frequent).reset_index()
most_freq_trans = most_freq_trans.dropna()
```

### The Differentiation Problem

**Why StringID matters in games:**

```
Same source "ì €ìž¥" â†’ Different targets based on context:
- UI_BUTTON_SAVE    â†’ "Save"
- UI_MENU_SAVE      â†’ "Save Game"
- DIALOGUE_SAVE_NPC â†’ "I'll keep it safe"
```

If we only use Source as the key, we lose this differentiation!

### PKL Structure Options

| Option | PKL Structure | Pros | Cons |
|--------|--------------|------|------|
| **A. Source-only key** | `{source: target}` | Simple, current approach | Loses StringID context |
| **B. Compound key** | `{(source, string_id): target}` | Preserves differentiation | Exact StringID match required |
| **C. Source key + metadata** | `{source: {target, string_id, ...}}` | Flexible lookup | Complex, last-write-wins for duplicates |
| **D. Source key + list** | `{source: [{target, string_id}, ...]}` | All variations preserved | Multiple results to handle |

### RECOMMENDATION: Option D (Source key + list of variations)

```python
# Proposed PKL structure
{
    "ì €ìž¥": [
        {"target": "Save", "string_id": "UI_BUTTON_SAVE", "context": "button"},
        {"target": "Save Game", "string_id": "UI_MENU_SAVE", "context": "menu"},
        {"target": "I'll keep it safe", "string_id": "DIALOGUE_SAVE_NPC", "context": "dialogue"}
    ],
    "ì·¨ì†Œ": [
        {"target": "Cancel", "string_id": "UI_CANCEL", "context": "button"}
    ],
    ...
}
```

### How Matching Would Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MATCHING WITH STRINGID                                    â”‚
â”‚                                                                             â”‚
â”‚  Input: Source="ì €ìž¥", StringID="UI_BUTTON_SAVE"                            â”‚
â”‚                                                                             â”‚
â”‚  Step 1: Embedding lookup for "ì €ìž¥" â†’ Find similar sources                â”‚
â”‚                                                                             â”‚
â”‚  Step 2: For each match, check variations:                                 â”‚
â”‚          â””â”€â”€ Match found: "ì €ìž¥" has 3 variations                          â”‚
â”‚                                                                             â”‚
â”‚  Step 3: StringID matching (if provided):                                  â”‚
â”‚          â”œâ”€â”€ Exact StringID match? â†’ Use that target (highest priority)   â”‚
â”‚          â”œâ”€â”€ Partial StringID match? â†’ Suggest as option                   â”‚
â”‚          â””â”€â”€ No StringID match? â†’ Return all variations for user choice   â”‚
â”‚                                                                             â”‚
â”‚  Result:                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Best Match: "Save" (StringID: UI_BUTTON_SAVE) â† Exact match         â”‚   â”‚
â”‚  â”‚ Alternatives:                                                        â”‚   â”‚
â”‚  â”‚   - "Save Game" (StringID: UI_MENU_SAVE)                            â”‚   â”‚
â”‚  â”‚   - "I'll keep it safe" (StringID: DIALOGUE_SAVE_NPC)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Embedding Array Alignment

**Important:** Embeddings array and PKL must stay aligned!

```python
# Current (simple):
embeddings[0] â†’ sources[0] â†’ pkl[sources[0]]

# With variations (proposed):
embeddings[0] â†’ sources[0] â†’ pkl[sources[0]][0]  # First variation
                           â†’ pkl[sources[0]][1]  # Second variation (SAME embedding!)
```

**Key insight:** We embed the SOURCE text, not the target. So multiple variations with the same source share ONE embedding.

```python
# Structure
sources = ["ì €ìž¥", "ì·¨ì†Œ", ...]           # Unique sources only
embeddings = [emb_ì €ìž¥, emb_ì·¨ì†Œ, ...]    # One embedding per unique source
pkl = {
    "ì €ìž¥": [variation1, variation2, ...],  # Multiple targets per source
    "ì·¨ì†Œ": [variation1],
    ...
}

# Alignment: len(sources) == len(embeddings) == len(pkl.keys())
# Variations are metadata, not separate embeddings
```

### Database Schema Consideration

```sql
-- Current (if exists)
CREATE TABLE tm_entries (
    id SERIAL PRIMARY KEY,
    tm_id INTEGER,
    source_text TEXT,
    target_text TEXT
);

-- Proposed (with StringID)
CREATE TABLE tm_entries (
    id SERIAL PRIMARY KEY,
    tm_id INTEGER,
    source_text TEXT,
    target_text TEXT,
    string_id TEXT,           -- NEW: Optional StringID
    context TEXT,             -- NEW: Optional context hint
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    UNIQUE(tm_id, source_text, string_id)  -- Compound unique constraint
);
```

### Technical Feasibility: YES - Here's Why

**Key Insight:** Embeddings are based on SOURCE TEXT only. StringID is just metadata for selection.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    END-TO-END STRINGID IMPLEMENTATION                        â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚  PHASE 1: TM CREATION (User uploads file with Source/Target/StringID)      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  Input File:                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Source       â”‚ Target            â”‚ StringID               â”‚            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚  â”‚ ì €ìž¥         â”‚ Save              â”‚ UI_BUTTON_SAVE         â”‚            â”‚
â”‚  â”‚ ì €ìž¥         â”‚ Save Game         â”‚ UI_MENU_SAVE           â”‚            â”‚
â”‚  â”‚ ì €ìž¥         â”‚ I'll keep it safe â”‚ DIALOGUE_SAVE_NPC      â”‚            â”‚
â”‚  â”‚ ì·¨ì†Œ         â”‚ Cancel            â”‚ UI_CANCEL              â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                             â”‚
â”‚  Step 1: Precheck (StringID Mode)                                          â”‚
â”‚  â””â”€â”€ All rows have Source + Target + StringID? â†’ YES, proceed             â”‚
â”‚                                                                             â”‚
â”‚  Step 2: Database Insert                                                   â”‚
â”‚  â””â”€â”€ INSERT INTO tm_entries (source_text, target_text, string_id)         â”‚
â”‚      4 rows inserted (NOT deduplicated - we keep all variations)          â”‚
â”‚                                                                             â”‚
â”‚  Step 3: Build Embeddings (source only, deduplicated)                      â”‚
â”‚  â””â”€â”€ Unique sources: ["ì €ìž¥", "ì·¨ì†Œ"] â†’ 2 embeddings                       â”‚
â”‚  â””â”€â”€ embeddings.npy shape: (2, 768)                                        â”‚
â”‚                                                                             â”‚
â”‚  Step 4: Build PKL with variations                                         â”‚
â”‚  â””â”€â”€ {                                                                     â”‚
â”‚          "ì €ìž¥": [                                                          â”‚
â”‚              {"target": "Save", "string_id": "UI_BUTTON_SAVE", "entry_id": 1},â”‚
â”‚              {"target": "Save Game", "string_id": "UI_MENU_SAVE", "entry_id": 2},â”‚
â”‚              {"target": "I'll keep it safe", "string_id": "DIALOGUE_SAVE_NPC", "entry_id": 3}â”‚
â”‚          ],                                                                â”‚
â”‚          "ì·¨ì†Œ": [                                                          â”‚
â”‚              {"target": "Cancel", "string_id": "UI_CANCEL", "entry_id": 4} â”‚
â”‚          ]                                                                 â”‚
â”‚      }                                                                     â”‚
â”‚                                                                             â”‚
â”‚  Result:                                                                   â”‚
â”‚  - embeddings.npy: 2 vectors (one per unique source)                       â”‚
â”‚  - pkl: 2 keys, but "ì €ìž¥" has 3 variations                                â”‚
â”‚  - Alignment: embeddings[0] â†’ "ì €ìž¥" â†’ pkl["ì €ìž¥"] (all 3 variations)      â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚  PHASE 2: PRETRANSLATION (User has new file to translate)                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  Input: New file with Source + StringID (no Target yet)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚ Source       â”‚ StringID               â”‚                                 â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                 â”‚
â”‚  â”‚ ì €ìž¥         â”‚ UI_BUTTON_SAVE         â”‚  â† Want to find translation    â”‚
â”‚  â”‚ ì €ìž¥         â”‚ NEW_SAVE_CONTEXT       â”‚  â† New StringID, no exact matchâ”‚
â”‚  â”‚ ìƒˆë¡œìš´       â”‚ UI_NEW                 â”‚  â† New source, no match at all â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                                                                             â”‚
â”‚  Query 1: Source="ì €ìž¥", StringID="UI_BUTTON_SAVE"                         â”‚
â”‚  â”œâ”€â”€ Step A: Embed "ì €ìž¥" â†’ vector                                         â”‚
â”‚  â”œâ”€â”€ Step B: FAISS search â†’ finds "ì €ìž¥" at 100% similarity               â”‚
â”‚  â”œâ”€â”€ Step C: Get variations from PKL â†’ 3 options                          â”‚
â”‚  â”œâ”€â”€ Step D: StringID exact match? â†’ YES! "UI_BUTTON_SAVE" exists         â”‚
â”‚  â””â”€â”€ Result: "Save" (exact StringID match, confidence=100%)               â”‚
â”‚                                                                             â”‚
â”‚  Query 2: Source="ì €ìž¥", StringID="NEW_SAVE_CONTEXT"                       â”‚
â”‚  â”œâ”€â”€ Step A: Embed "ì €ìž¥" â†’ vector                                         â”‚
â”‚  â”œâ”€â”€ Step B: FAISS search â†’ finds "ì €ìž¥" at 100% similarity               â”‚
â”‚  â”œâ”€â”€ Step C: Get variations from PKL â†’ 3 options                          â”‚
â”‚  â”œâ”€â”€ Step D: StringID exact match? â†’ NO, "NEW_SAVE_CONTEXT" not found     â”‚
â”‚  â””â”€â”€ Result: Return ALL 3 variations for user to choose                   â”‚
â”‚      [                                                                     â”‚
â”‚        {"target": "Save", "string_id": "UI_BUTTON_SAVE"},                 â”‚
â”‚        {"target": "Save Game", "string_id": "UI_MENU_SAVE"},              â”‚
â”‚        {"target": "I'll keep it safe", "string_id": "DIALOGUE_SAVE_NPC"} â”‚
â”‚      ]                                                                     â”‚
â”‚                                                                             â”‚
â”‚  Query 3: Source="ìƒˆë¡œìš´", StringID="UI_NEW"                               â”‚
â”‚  â”œâ”€â”€ Step A: Embed "ìƒˆë¡œìš´" â†’ vector                                       â”‚
â”‚  â”œâ”€â”€ Step B: FAISS search â†’ no match above threshold                      â”‚
â”‚  â””â”€â”€ Result: No match found                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This is Technically Doable

| Concern | Answer |
|---------|--------|
| **Embedding alignment?** | No change needed! Embeddings are per unique SOURCE. Variations are metadata in PKL. |
| **FAISS search change?** | No change! Search by source embedding, then filter by StringID in post-processing. |
| **Database change?** | Minor: Add `string_id` column to `tm_entries` table. |
| **PKL format change?** | Yes: Change from `{source: target}` to `{source: [variations]}` |
| **Backward compatible?** | Yes: If `string_id` is NULL, behave like current system (single target per source). |

### Implementation Steps

```python
# Step 1: Modify TM entry model (database)
class LDMTMEntry(Base):
    id = Column(Integer, primary_key=True)
    tm_id = Column(Integer, ForeignKey('ldm_tms.id'))
    source_text = Column(Text, nullable=False)
    target_text = Column(Text, nullable=False)
    string_id = Column(Text, nullable=True)  # NEW - optional

# Step 2: Modify PKL builder (tm_indexer.py)
def _build_whole_lookup_with_stringid(self, entries):
    lookup = {}
    for entry in entries:
        source = normalize_for_hash(entry["source_text"])
        if source not in lookup:
            lookup[source] = []
        lookup[source].append({
            "entry_id": entry["id"],
            "target_text": entry["target_text"],
            "string_id": entry.get("string_id")
        })
    return lookup

# Step 3: Modify search to handle variations (tm_indexer.py)
def search_with_stringid(self, source: str, string_id: str = None):
    # Get all variations for this source
    variations = self.lookup.get(normalize_for_hash(source), [])

    if not variations:
        return None

    # If StringID provided, try exact match first
    if string_id:
        for var in variations:
            if var.get("string_id") == string_id:
                return {"match": var, "exact_stringid": True}

    # No exact StringID match - return all variations
    return {"match": variations[0], "alternatives": variations, "exact_stringid": False}
```

### Open Questions

- [ ] What if user doesn't provide StringID in the file? (Default to NULL, match by source only?)
- [ ] Should StringID matching be exact or fuzzy? (UI_BUTTON_SAVE vs UI_BTN_SAVE)
- [ ] How to handle TM uploads from files without StringID column?
- [ ] UI: How to display multiple variations to user?

---

## 3. Excel to TM Creation Flow

### Supported Excel Structures

**Two valid structures:**

```
STRUCTURE A: Source + Target (Standard TM)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source           â”‚ Target           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì €ìž¥             â”‚ Save             â”‚
â”‚ ì·¨ì†Œ             â”‚ Cancel           â”‚
â”‚ í™•ì¸             â”‚ OK               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ General purpose TM
â†’ Duplicates: most frequent target wins
â†’ Simple 1:1 sourceâ†’target matching


STRUCTURE B: Source + Target + StringID (Precise TM)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source           â”‚ Target           â”‚ StringID         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì €ìž¥             â”‚ Save             â”‚ UI_BUTTON_SAVE   â”‚
â”‚ ì €ìž¥             â”‚ Save Game        â”‚ UI_MENU_SAVE     â”‚
â”‚ ì €ìž¥             â”‚ I'll keep it     â”‚ DIALOGUE_NPC     â”‚
â”‚ ì·¨ì†Œ             â”‚ Cancel           â”‚ UI_CANCEL        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ Context-aware TM (game UI vs dialogue)
â†’ Keeps ALL variations (no deduplication)
â†’ StringID matching for precise results
```

### User Flow: Excel â†’ TM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXCEL TO TM CREATION FLOW                                 â”‚
â”‚                                                                             â”‚
â”‚  Step 1: User right-clicks Excel file in File Explorer                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚  ðŸ“¥ Download File               â”‚                                        â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                        â”‚
â”‚  â”‚  ðŸ“š Create TM from this file... â”‚ â† User clicks this                    â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                        â”‚
â”‚  â”‚  ðŸ” Run Full QA Check           â”‚                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                             â”‚
â”‚  Step 2: TM Creation Modal appears                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     CREATE TM FROM EXCEL                             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Source File: game_strings_2025.xlsx                                â”‚   â”‚
â”‚  â”‚  Detected Rows: 12,450                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â•â•â• TM NAME â•â•â•                                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Name: [ BDO_UI_Terms_v1                    ]                       â”‚   â”‚
â”‚  â”‚        âœ… Name is valid                                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â•â•â• TM MODE â•â•â•                                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â—‹ Standard Mode (Source + Target)                                  â”‚   â”‚
â”‚  â”‚    â””â”€â”€ General purpose, duplicates merged                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â— StringID Mode (Source + Target + StringID)                       â”‚   â”‚
â”‚  â”‚    â””â”€â”€ Precise matching, keeps all variations                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â•â•â• COLUMN MAPPING â•â•â•                                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Source Column:   [ A â–¼ ]  Preview: ì €ìž¥, ì·¨ì†Œ, í™•ì¸...             â”‚   â”‚
â”‚  â”‚  Target Column:   [ B â–¼ ]  Preview: Save, Cancel, OK...             â”‚   â”‚
â”‚  â”‚  StringID Column: [ C â–¼ ]  Preview: UI_BTN_SAVE, UI_CANCEL...       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Step 3: Data Validation (automatic)                                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â•â•â• DATA VALIDATION â•â•â•                                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Standard Mode:                         StringID Mode:              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚ Total rows: 12,450          â”‚       â”‚ Total rows: 12,450      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ Valid rows: 12,380 âœ…       â”‚       â”‚ Valid rows: 12,450 âœ…   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ Skipped (no target): 70     â”‚       â”‚ Missing Source: 0       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ After dedup: 8,920          â”‚       â”‚ Missing Target: 0       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚                             â”‚       â”‚ Missing StringID: 0     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ Ready to process âœ…         â”‚       â”‚ Ready to process âœ…     â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  âš ï¸ StringID Mode ERROR example:                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ âŒ Cannot process in StringID Mode                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Data has discrepancies:                                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ - Rows missing Source: 5                                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ - Rows missing Target: 23                                   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ - Rows missing StringID: 142                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Please clean your data or use Standard Mode instead.        â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚                        [Cancel]  [Create TM]                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Step 4: Processing (background task)                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Creating TM "BDO_UI_Terms_v1"...                                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 68%                                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Stage: Building embeddings                                         â”‚   â”‚
â”‚  â”‚  Progress: 8,500 / 12,450 entries                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Step 5: TM appears in TM Explorer                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ðŸ“ Translation Memories                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ðŸ“š BDO_EN_Main (45,230 entries)                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ðŸ“š BDO_KR_Reference (32,100 entries)                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ðŸ“š BDO_UI_Terms_v1 (12,450 entries) [StringID] â† NEW          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### TM Naming Validation

```python
def validate_tm_name(name: str) -> tuple[bool, str]:
    """
    Validate TM name before creation.

    Rules:
    1. Not empty
    2. 3-50 characters
    3. Only: letters, numbers, underscore, hyphen, space
    4. No leading/trailing spaces
    5. Not duplicate of existing TM

    Returns: (is_valid, error_message)
    """
    if not name or not name.strip():
        return False, "Name cannot be empty"

    name = name.strip()

    if len(name) < 3:
        return False, "Name must be at least 3 characters"

    if len(name) > 50:
        return False, "Name must be 50 characters or less"

    import re
    if not re.match(r'^[a-zA-Z0-9_\- ]+$', name):
        return False, "Only letters, numbers, underscore, hyphen, space allowed"

    # Check duplicate
    existing = get_existing_tm_names()
    if name.lower() in [n.lower() for n in existing]:
        return False, f"TM '{name}' already exists"

    return True, ""
```

### Data Validation by Mode

```python
def validate_excel_for_tm(df: pd.DataFrame, mode: str) -> dict:
    """
    Validate Excel data before TM creation.

    Args:
        df: DataFrame with Source, Target, (optional StringID)
        mode: "standard" or "stringid"

    Returns:
        {
            "valid": bool,
            "total_rows": int,
            "valid_rows": int,
            "errors": list,
            "warnings": list
        }
    """
    result = {
        "valid": True,
        "total_rows": len(df),
        "valid_rows": 0,
        "errors": [],
        "warnings": []
    }

    if mode == "standard":
        # Lenient: skip rows without source OR target
        valid_mask = df['source'].notna() & df['target'].notna()
        valid_mask &= (df['source'] != '') & (df['target'] != '')
        result["valid_rows"] = valid_mask.sum()
        result["skipped"] = len(df) - result["valid_rows"]

        if result["skipped"] > 0:
            result["warnings"].append(f"{result['skipped']} rows skipped (missing source/target)")

    elif mode == "stringid":
        # Strict: ALL rows must have source + target + stringid
        missing_source = df['source'].isna().sum() + (df['source'] == '').sum()
        missing_target = df['target'].isna().sum() + (df['target'] == '').sum()
        missing_stringid = df['string_id'].isna().sum() + (df['string_id'] == '').sum()

        if missing_source > 0 or missing_target > 0 or missing_stringid > 0:
            result["valid"] = False
            result["errors"].append(f"Missing Source: {missing_source}")
            result["errors"].append(f"Missing Target: {missing_target}")
            result["errors"].append(f"Missing StringID: {missing_stringid}")
        else:
            result["valid_rows"] = len(df)

    return result
```

---

## 4. Glossary Creation Flow

**Translation of entries requires external API â†’ FUTURE feature**

### Flow Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GLOSSARY CREATION FLOW                                    â”‚
â”‚                                                                             â”‚
â”‚  Step 1: User right-clicks file â†’ "Create Glossary..."                     â”‚
â”‚                                                                             â”‚
â”‚  Step 2: Modal appears                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              CREATE GLOSSARY                                         â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Source File: game_strings_2025.xlsx                                â”‚   â”‚
â”‚  â”‚  Rows: 12,450                                                       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Glossary Name: [ BDO_UI_Terms_v1              ]                    â”‚   â”‚
â”‚  â”‚                  â””â”€â”€ Validation: no duplicates, valid chars         â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â•â•â• EXTRACTION RULES â•â•â•                                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Max length:    [ 26 ] characters                                   â”‚   â”‚
â”‚  â”‚  [ ] Include sentences (ending with . ! ?)                          â”‚   â”‚
â”‚  â”‚  [x] Skip duplicates (same source text)                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  â•â•â• PREVIEW â•â•â•                                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Unique terms found: 2,340                                          â”‚   â”‚
â”‚  â”‚  After filtering: 1,892                                             â”‚   â”‚
â”‚  â”‚  Already in existing TMs: 1,450                                     â”‚   â”‚
â”‚  â”‚  NEW terms to add: 442                                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚                        [Cancel]  [Create Glossary]                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Step 3: Processing (background task with progress)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Creating glossary "BDO_UI_Terms_v1"...                             â”‚   â”‚
â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 62%                                           â”‚   â”‚
â”‚  â”‚  Processing: 275/442 terms                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Step 4: Glossary appears in TM Explorer                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ðŸ“ Translation Memories                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ðŸ“š BDO_EN_Main (45,230 entries)                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ðŸ“š BDO_KR_Reference (32,100 entries)                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ðŸ“ BDO_UI_Terms_v1 (442 entries) â† NEW (source-only)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Naming Validation

```python
def validate_glossary_name(name: str) -> tuple[bool, str]:
    """
    Validate glossary name before creation.

    Rules:
    - Not empty
    - 3-50 characters
    - Only alphanumeric, underscore, hyphen, space
    - No leading/trailing spaces
    - Not a duplicate of existing TM/glossary name

    Returns: (is_valid, error_message)
    """
    # Check empty
    if not name or not name.strip():
        return False, "Name cannot be empty"

    name = name.strip()

    # Check length
    if len(name) < 3:
        return False, "Name must be at least 3 characters"
    if len(name) > 50:
        return False, "Name must be 50 characters or less"

    # Check characters
    import re
    if not re.match(r'^[a-zA-Z0-9_\- ]+$', name):
        return False, "Name can only contain letters, numbers, underscore, hyphen, and space"

    # Check duplicate
    existing = get_existing_tm_names()
    if name.lower() in [n.lower() for n in existing]:
        return False, f"A TM/Glossary named '{name}' already exists"

    return True, ""
```

### Glossary Entry Structure

```python
# Source-only glossary entry (target filled later)
{
    "source_text": "ì €ìž¥í•˜ê¸°",
    "target_text": None,           # NULL - to be filled later
    "string_id": "UI_BUTTON_SAVE", # Optional - if available in source file
    "status": "untranslated",      # untranslated | translated | reviewed
    "created_at": "2025-12-17T11:00:00Z",
    "source_file": "game_strings_2025.xlsx"
}
```

### Open Questions

- [ ] Should glossaries be visually different from TMs in the explorer? (different icon?)
- [ ] Can user edit entries directly in the glossary? (add target manually)
- [ ] How to merge a glossary into an existing TM?
- [ ] Should we track which file the term came from?

---

## 4. Data Preprocessing Strategy

### EXISTING ROBUST LOGIC (Already Implemented!)

We already have comprehensive data preprocessing across both XLS Transfer and LDM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXISTING PREPROCESSING LOGIC                              â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚  â•‘ XLS TRANSFER (process_operation.py)                                    â•‘ â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚  â•‘ Line 85:  df.dropna()           â†’ Skip rows without source OR target  â•‘ â”‚
â”‚  â•‘ Line 87:  clean_text()          â†’ Remove _x000D_, strip whitespace    â•‘ â”‚
â”‚  â•‘ Line 112: groupby.agg(most_freq) â†’ Most frequent translation wins     â•‘ â”‚
â”‚  â•‘ Line 112: dropna()              â†’ Remove any remaining NaN values     â•‘ â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚  â•‘ LDM TM MANAGER (tm_manager.py)                                         â•‘ â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚  â•‘ Line 431: Filter comprehension  â†’ Only keep entries with BOTH         â•‘ â”‚
â”‚  â•‘           source AND target present (skip empty)                       â•‘ â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚  â•‘ LDM TM INDEXER (tm_indexer.py)                                         â•‘ â”‚
â”‚  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£ â”‚
â”‚  â•‘ Line 370: if not source: continue     â†’ Skip entries without source   â•‘ â”‚
â”‚  â•‘ Line 378: if not line.strip(): continue â†’ Skip empty lines            â•‘ â”‚
â”‚  â•‘ Line 430: if not source: continue     â†’ Skip for embeddings           â•‘ â”‚
â”‚  â•‘ Line 434: if normalized:              â†’ Only embed non-empty text     â•‘ â”‚
â”‚  â•‘ Line 102: normalize_for_embedding()   â†’ Normalize whitespace          â•‘ â”‚
â”‚  â•‘ Line 89:  normalize_for_hash()        â†’ Lowercase + normalize         â•‘ â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Summary: What's Already Handled

| Issue | XLS Transfer | LDM | Status |
|-------|-------------|-----|--------|
| Empty source | `dropna()` | `if not source: continue` | âœ… HANDLED |
| Empty target | `dropna()` | Filter comprehension | âœ… HANDLED |
| Empty lines | N/A | `if not line.strip(): continue` | âœ… HANDLED |
| Control chars | `clean_text()` | N/A (clean on upload) | âœ… HANDLED |
| Whitespace | `clean_text()` | `normalize_for_embedding()` | âœ… HANDLED |
| Duplicate sources | Most frequent wins | First occurrence wins | âœ… HANDLED |

### Code References

**XLS Transfer - process_operation.py:85-112:**
```python
# Skip rows without translation
df = df.iloc[:, [col_kr_index, col_fr_index]].dropna()

# Clean text
kr_texts = df.iloc[:, 0].apply(clean_text).tolist()
fr_texts = df.iloc[:, 1].apply(clean_text).tolist()

# Most frequent translation wins
most_freq_trans = df.groupby('KR')['FR'].agg(safe_most_frequent).reset_index()
most_freq_trans = most_freq_trans.dropna()
```

**LDM - tm_manager.py:425-432:**
```python
# Only keep entries with BOTH source AND target
formatted_entries = [
    {"source_text": e.get("source") or e.get("source_text"),
     "target_text": e.get("target") or e.get("target_text")}
    for e in entries
    if (e.get("source") or e.get("source_text")) and (e.get("target") or e.get("target_text"))
]
```

**LDM - tm_indexer.py:370-384:**
```python
for entry in entries:
    source = entry["source_text"]
    if not source:
        continue  # Skip entries without source

    for i, line in enumerate(source_lines):
        if not line.strip():
            continue  # Skip empty lines

        normalized_line = normalize_for_hash(line)
        if not normalized_line:
            continue  # Skip if normalization produces empty string
```

### What This Means for P36

**We DON'T need to re-implement preprocessing!**

The existing logic already:
1. Skips rows without source or target
2. Cleans control characters and whitespace
3. Handles duplicates (most frequent or first wins)
4. Normalizes text for both hash and embedding matching

### Additional Preprocessing (Optional Enhancement)

The only NEW preprocessing we might add from WebTranslatorNew is:

| Feature | Source | Benefit |
|---------|--------|---------|
| DB duplicate check BEFORE embedding | `preprocessor.py` | Skip already-in-TM entries |

```python
# OPTIONAL: Check if entry already exists in TM before processing
existing_pairs = {(e.source_text, e.target_text) for e in existing_entries}
new_entries = [e for e in entries if (e.source, e.target) not in existing_pairs]
```

This is an OPTIMIZATION, not a requirement. We can add it later if needed.

### Preprocessing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PREPROCESSING FLOW                                   â”‚
â”‚                                                                             â”‚
â”‚  Input: 1,000 rows from uploaded file                                      â”‚
â”‚                                                                             â”‚
â”‚  Step 1: Remove empty rows                                                 â”‚
â”‚          â”œâ”€â”€ 1,000 rows                                                    â”‚
â”‚          â””â”€â”€ 985 rows (15 empty removed)                                   â”‚
â”‚                                                                             â”‚
â”‚  Step 2: Clean whitespace and control chars                                â”‚
â”‚          â””â”€â”€ 985 rows (cleaned in-place)                                   â”‚
â”‚                                                                             â”‚
â”‚  Step 3: Resolve duplicates within file                                    â”‚
â”‚          â”œâ”€â”€ 985 rows                                                      â”‚
â”‚          â”œâ”€â”€ Found 45 duplicate sources                                    â”‚
â”‚          â””â”€â”€ 940 unique entries                                            â”‚
â”‚                                                                             â”‚
â”‚  Step 4: Check against existing TM                                         â”‚
â”‚          â”œâ”€â”€ 940 entries                                                   â”‚
â”‚          â”œâ”€â”€ 720 already exist (exact source+target match)                â”‚
â”‚          â””â”€â”€ 220 NEW entries to add                                        â”‚
â”‚                                                                             â”‚
â”‚  Output: 220 entries ready for embedding generation                        â”‚
â”‚                                                                             â”‚
â”‚  Summary shown to user:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Upload Summary:                                                     â”‚   â”‚
â”‚  â”‚  - Total rows: 1,000                                                â”‚   â”‚
â”‚  â”‚  - Empty removed: 15                                                â”‚   â”‚
â”‚  â”‚  - Duplicates merged: 45                                            â”‚   â”‚
â”‚  â”‚  - Already in TM: 720                                               â”‚   â”‚
â”‚  â”‚  - NEW to add: 220                                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  [Cancel]  [Add 220 new entries]                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Open Questions

- [ ] Should preprocessing happen on upload or on-demand?
- [ ] How to handle encoding issues (UTF-8 BOM, etc.)?
- [ ] Should we log what was removed/merged for audit?

---

## Summary of Recommendations

| Topic | Recommendation | Status |
|-------|----------------|--------|
| **Batch Processing** | Chunked (500 rows) + Multiprocessing (4 workers) | TO IMPLEMENT |
| **StringID Handling** | User choice: Standard (lenient) vs StringID (strict precheck) | TO IMPLEMENT |
| **Glossary Creation** | User names first, validates, appears in TM explorer | TO IMPLEMENT |
| **Data Preprocessing** | Already robust! Skip empty, clean text, handle duplicates | âœ… EXISTS |

---

## Key Takeaways

### What's Already Done (Don't Re-implement!)

1. **Data Cleaning** - `clean_text()`, `dropna()`, skip empty rows
2. **Duplicate Handling** - Most frequent translation wins (XLS), first wins (LDM)
3. **Normalization** - `normalize_for_hash()`, `normalize_for_embedding()`
4. **5-Tier Cascade** - Hash â†’ FAISS HNSW â†’ N-gram (already in tm_indexer.py)

### What's New to Implement

1. **StringID Mode** - Strict precheck, compound key with variations
2. **Batch Optimization** - Chunked + Multiprocessing for large files
3. **Glossary Creation** - Right-click â†’ name â†’ extract to TM explorer
4. **Unified Pretranslation API** - `/api/ldm/pretranslate` with engine selection

---

## Next Steps

1. [ ] Review and approve these design decisions
2. [ ] Address open questions (especially StringID matching strategy)
3. [ ] Create detailed API specs based on decisions
4. [ ] Begin implementation

---

*Created: 2025-12-17*
*Status: AWAITING REVIEW*
